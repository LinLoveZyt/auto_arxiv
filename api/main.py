# api/main.py


import logging
import asyncio
import json
import os
from contextlib import asynccontextmanager
from typing import Any, AsyncGenerator, Dict, List
from datetime import datetime, time
from pathlib import Path


from fastapi import FastAPI, HTTPException, Body
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, RedirectResponse, StreamingResponse
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from starlette.concurrency import iterate_in_threadpool

from core.bootstrap import initialize_core_services
from hrag.vector_db import vector_db_manager
from api import schemas
from workflows import daily_flow, query_flow
from data_ingestion import arxiv_fetcher
from hrag import metadata_db
from core import config as config_module
from agents import ingestion_agent
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Executes initialization on app startup and cleanup on shutdown."""
    logger.info("ğŸš€ API service starting up...")
    # åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒæœåŠ¡
    await run_in_threadpool(initialize_core_services)
    
    # â–¼â–¼â–¼ [ä¿®æ”¹] â–¼â–¼â–¼
    # æ­¤å¤„å·²ç§»é™¤æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨è¿è¡Œæ¯æ—¥ä»»åŠ¡çš„é€»è¾‘ã€‚
    # ç°åœ¨æ¯æ—¥ä»»åŠ¡åªèƒ½é€šè¿‡APIæ¥å£ /api/run/daily_workflow æ‰‹åŠ¨è§¦å‘ã€‚
    logger.info("Server started. Daily workflow will NOT run automatically.")
    
    yield
    
    logger.info("ğŸ›‘ API service shutting down...")
    if vector_db_manager:
        await run_in_threadpool(vector_db_manager.save)
        logger.info("âœ… FAISS index saved successfully.")

app = FastAPI(
    title="auto_arvix API",
    description="An intelligent system API for automated processing and querying of arXiv papers.",
    version="8.0.0 (Preference Edition)",
    lifespan=lifespan,
    docs_url=None,
    redoc_url=None
)

app.mount("/ui", StaticFiles(directory="web"), name="ui")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

@app.get("/", include_in_schema=False)
async def root(): 
    return RedirectResponse(url="/ui/index.html")

@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html(): 
    return get_swagger_ui_html(openapi_url="/openapi.json", title=app.title)

@app.get("/openapi.json", include_in_schema=False)
async def get_open_api_endpoint(): 
    return get_openapi(title=app.title, version=app.version, routes=app.routes)

# --- Workflow Triggers ---
@app.post("/api/run/daily_workflow", response_model=schemas.DailyRunResponse, tags=["Workflows"])
async def trigger_daily_run():
    result = await run_in_threadpool(daily_flow.run_daily_workflow)
    return schemas.DailyRunResponse(**result)

@app.post("/api/run/category_collection", response_model=schemas.GeneralStatusResponse, tags=["Workflows"])
async def trigger_category_collection():
    """
    æ‰‹åŠ¨è§¦å‘ä¸€æ¬¡è½»é‡çº§çš„ç±»åˆ«æ”¶é›†ä»»åŠ¡ã€‚
    è¯¥ä»»åŠ¡ä¼šä»arXivè·å–è®ºæ–‡ï¼Œä»…è¿›è¡ŒAIåˆ†ç±»ä»¥æ‰©å……ç³»ç»ŸçŸ¥è¯†ï¼Œä½†ä¸ä¼šä¸‹è½½PDFæˆ–å…¥åº“ã€‚
    """
    logger.info("--- [API] æ¥æ”¶åˆ°æ‰‹åŠ¨ç±»åˆ«æ”¶é›†è¯·æ±‚ ---")
    # æ³¨æ„ï¼šæˆ‘ä»¬å¤ç”¨ schemas.GeneralStatusResponse ä½œä¸ºå“åº”æ¨¡å‹ï¼Œå› ä¸ºå®ƒè¶³å¤Ÿé€šç”¨
    # æˆ‘ä»¬è°ƒç”¨æ–°åˆ›å»ºçš„å·¥ä½œæµå‡½æ•°
    result = await run_in_threadpool(daily_flow.run_category_collection_workflow)
    return schemas.GeneralStatusResponse(message=result.get("message", "æ“ä½œå®Œæˆã€‚"))


@app.get("/api/settings/available-models", response_model=List[str], tags=["Configuration"])
async def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„LLMæ¨¡å‹åˆ—è¡¨ã€‚"""
    return config_module.get_current_config().get("AVAILABLE_LLM_MODELS", [])


@app.post("/api/query", tags=["Core Functionality"])
async def handle_query_stream(request: schemas.QueryRequest):
    """
    Handles user's natural language query with a streaming response for real-time progress.
    """
    logger.info(f"Received streaming query request: '{request.query_text[:50]}...' (Online: {request.online_search_enabled})")

    workflow_instance = query_flow.QueryWorkflow()

    # æ³¨æ„ï¼šè¿™é‡Œä¸å†ä¼ é€’ model_name
    sync_generator = workflow_instance.run_stream(
        query_text=request.query_text, 
        online_search_enabled=request.online_search_enabled
    )

    async_generator = iterate_in_threadpool(sync_generator)

    async def final_streamer():
        try:
            async for update in async_generator:
                yield f"data: {json.dumps(update)}\n\n"
        except Exception as e:
            logger.critical(f"Unhandled exception during stream processing: {e}", exc_info=True)
            error_update = {"type": "error", "message": "An internal server error occurred while processing your request."}
            yield f"data: {json.dumps(error_update)}\n\n"

    return StreamingResponse(final_streamer(), media_type="text/event-stream")


@app.get("/api/reports", response_model=List[str], tags=["Reports"])
async def list_reports():
    """Lists all available daily PDF reports."""
    if not config_module.REPORTS_DIR.exists():
        return []
    try:
        # Sort files by name, which should roughly correspond to date
        files = sorted([f.name for f in config_module.REPORTS_DIR.iterdir() if f.suffix == '.pdf'], reverse=True)
        return files
    except Exception as e:
        logger.error(f"Failed to list reports: {e}")
        return []

        
@app.get("/api/reports/{filename}", tags=["Reports"])
async def get_report_pdf(filename: str):
    """Serves a specific report PDF file."""
    # Security: Ensure the filename is safe
    if ".." in filename or filename.startswith("/"):
        raise HTTPException(status_code=400, detail="Invalid filename.")
        
    report_path = config_module.REPORTS_DIR / filename
    if not report_path.is_file():
        raise HTTPException(status_code=404, detail="Report file not found.")
        
    return FileResponse(report_path, media_type='application/pdf', filename=filename)


# --- File Services ---
@app.get("/papers/pdf/{arxiv_id}", tags=["File Services"])
async def get_paper_pdf(arxiv_id: str):
    details = metadata_db.get_paper_details_by_id(arxiv_id)
    if not details or not details.get("pdf_path"): raise HTTPException(status_code=404, detail="PDF path not found")
    pdf_path = Path(details["pdf_path"])
    if not pdf_path.is_file(): raise HTTPException(status_code=404, detail="PDF file does not exist")
    return FileResponse(pdf_path, media_type='application/pdf', filename=f"{arxiv_id}.pdf")



@app.get("/api/categories", response_model=Dict[str, Any], tags=["Configuration"])
async def get_all_categories():
    """è·å–æ‰€æœ‰å·²çŸ¥çš„é¢†åŸŸå’Œä»»åŠ¡åˆ†ç±»ã€‚"""
    if not config_module.CATEGORIES_JSON_PATH.exists():
        return {}
    try:
        with open(config_module.CATEGORIES_JSON_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å–åˆ†ç±»æ–‡ä»¶: {e}")
        raise HTTPException(status_code=500, detail="Could not read categories file.")

@app.get("/api/user/preferences", response_model=Dict[str, Any], tags=["Configuration"])
async def get_user_preferences():
    """è·å–ç”¨æˆ·å½“å‰å·²é€‰æ‹©çš„åå¥½åˆ†ç±»ã€‚"""
    if not config_module.USER_PREFERENCES_PATH.exists():
        return {"selected_categories": []}
    try:
        with open(config_module.USER_PREFERENCES_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"æ— æ³•è¯»å–ç”¨æˆ·åå¥½æ–‡ä»¶: {e}")
        raise HTTPException(status_code=500, detail="Could not read user preferences file.")

@app.post("/api/user/preferences", response_model=Dict[str, str], tags=["Configuration"])
async def save_user_preferences(payload: Dict[str, Any] = Body(...)):
    """ä¿å­˜ç”¨æˆ·é€‰æ‹©çš„åå¥½åˆ†ç±»ã€‚"""
    # â–¼â–¼â–¼ [æ ¸å¿ƒä¿®æ”¹] å¢åŠ æ˜¾å¼æ—¥å¿—å’Œæ›´å¥å£®çš„æ–‡ä»¶å†™å…¥é€»è¾‘ â–¼â–¼â–¼
    logger.info(f"--- [æ¥æ”¶åˆ°åå¥½ä¿å­˜è¯·æ±‚] --- å‡†å¤‡ä¿å­˜ç”¨æˆ·åå¥½...")
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨ï¼Œä½œä¸ºä¸€é“é¢å¤–çš„ä¿é™©
    try:
        config_module.STORAGE_DIR.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(f"åœ¨ä¿å­˜åå¥½å‰ï¼Œæ— æ³•åˆ›å»ºæˆ–ç¡®è®¤ storage ç›®å½•: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼šæ— æ³•è®¿é—®å­˜å‚¨ç›®å½•ã€‚")

    if "selected_categories" not in payload:
        logger.error(f"æ”¶åˆ°çš„åå¥½è®¾ç½®è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ 'selected_categories' é”®ã€‚Payload: {payload}")
        raise HTTPException(status_code=400, detail="è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ã€‚")

    try:
        with open(config_module.USER_PREFERENCES_PATH, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=4, ensure_ascii=False)
        
        num_selected = len(payload.get("selected_categories", []))
        logger.info(f"âœ… --- [åå¥½ä¿å­˜æˆåŠŸ] --- ç”¨æˆ·åå¥½å·²å†™å…¥ user_preferences.jsonã€‚å…±ä¿å­˜ {num_selected} æ¡åå¥½ã€‚")
        return {"message": f"åå¥½ä¿å­˜æˆåŠŸï¼å…±ä¿å­˜ {num_selected} æ¡ã€‚"}
        
    except Exception as e:
        logger.critical(f"âŒ å†™å…¥ç”¨æˆ·åå¥½æ–‡ä»¶æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼šæ— æ³•å†™å…¥åå¥½æ–‡ä»¶ã€‚")


@app.post("/api/categories/propose-merges", response_model=Dict[str, Any], tags=["Maintenance"])
async def propose_category_merges():
    """
    è§¦å‘AIåˆ†æç°æœ‰åˆ†ç±»ï¼Œå¹¶è¿”å›ä¸€ä¸ªåˆå¹¶å»ºè®®åˆ—è¡¨ã€‚
    """
    logger.info("--- [API] æ¥æ”¶åˆ°åˆ†ç±»åˆå¹¶å»ºè®®è¯·æ±‚ ---")
    try:
        proposals = await run_in_threadpool(ingestion_agent.propose_category_merges)
        if proposals is None:
            raise HTTPException(status_code=500, detail="AI Agentæœªèƒ½æˆåŠŸç”Ÿæˆå»ºè®®ã€‚")
        return proposals
    except Exception as e:
        logger.critical(f"ç”Ÿæˆåˆå¹¶å»ºè®®æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ã€‚")


@app.get("/api/settings/global", response_model=Dict[str, Any], tags=["Configuration"])
async def get_global_settings():
    """è·å–å½“å‰æ‰€æœ‰å¯åŠ¨æ€é…ç½®çš„å…¨å±€å‚æ•°ã€‚"""
    return config_module.get_current_config()

@app.post("/api/settings/global", response_model=Dict[str, str], tags=["Configuration"])
async def update_global_settings(payload: Dict[str, Any] = Body(...)):
    """æ›´æ–°å…¨å±€å‚æ•°ï¼Œå¹¶å°†å…¶å†™å…¥è¦†ç›–æ–‡ä»¶ã€‚"""
    try:
        current_override = {}
        if config_module.CONFIG_OVERRIDE_PATH.exists():
            with open(config_module.CONFIG_OVERRIDE_PATH, 'r', encoding='utf-8') as f:
                current_override = json.load(f)
        
        current_override.update(payload)

        with open(config_module.CONFIG_OVERRIDE_PATH, 'w', encoding='utf-8') as f:
            json.dump(current_override, f, indent=4)
        
        logger.info(f"å…¨å±€é…ç½®å·²æˆåŠŸæ›´æ–°ï¼Œå†…å®¹: {payload}")
        return {"message": "å…¨å±€é…ç½®æ›´æ–°æˆåŠŸã€‚æ–°è®¾ç½®å°†åœ¨ä¸‹ä¸€æ¬¡ä»»åŠ¡è¿è¡Œæ—¶ç”Ÿæ•ˆã€‚"}
    except Exception as e:
        logger.error(f"æ›´æ–°å…¨å±€é…ç½®æ–‡ä»¶æ—¶å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="å†™å…¥é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")


@app.get("/api/settings/global", response_model=Dict[str, Any], tags=["Configuration"])
async def get_global_settings():
    """è·å–å½“å‰æ‰€æœ‰å¯åŠ¨æ€é…ç½®çš„å…¨å±€å‚æ•°ã€‚"""
    return config_module.get_current_config()

@app.post("/api/settings/global", response_model=Dict[str, str], tags=["Configuration"])
async def update_global_settings(payload: Dict[str, Any] = Body(...)):
    """æ›´æ–°å…¨å±€å‚æ•°ï¼Œå¹¶å°†å…¶å†™å…¥è¦†ç›–æ–‡ä»¶ã€‚"""
    try:
        # è¯»å–ç°æœ‰çš„è¦†ç›–æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸
        current_override = {}
        if config_module.CONFIG_OVERRIDE_PATH.exists():
            with open(config_module.CONFIG_OVERRIDE_PATH, 'r', encoding='utf-8') as f:
                current_override = json.load(f)
        
        # ä½¿ç”¨ä¼ å…¥çš„payloadæ›´æ–°é…ç½®
        current_override.update(payload)

        # å°†æ›´æ–°åçš„é…ç½®å†™å›è¦†ç›–æ–‡ä»¶
        with open(config_module.CONFIG_OVERRIDE_PATH, 'w', encoding='utf-8') as f:
            json.dump(current_override, f, indent=4)
        
        logger.info(f"å…¨å±€é…ç½®å·²æˆåŠŸæ›´æ–°ï¼Œå†…å®¹: {payload}")
        return {"message": "å…¨å±€é…ç½®æ›´æ–°æˆåŠŸã€‚æ–°è®¾ç½®å°†åœ¨ä¸‹ä¸€æ¬¡ä»»åŠ¡è¿è¡Œæ—¶ç”Ÿæ•ˆã€‚"}
    except Exception as e:
        logger.error(f"æ›´æ–°å…¨å±€é…ç½®æ–‡ä»¶æ—¶å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="å†™å…¥é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

@app.post("/api/categories/execute-merges", response_model=Dict[str, str], tags=["Maintenance"])
async def execute_category_merges(payload: Dict[str, Any] = Body(...)):
    """
    æ ¹æ®ç”¨æˆ·ç¡®è®¤çš„åˆ—è¡¨ï¼Œä½¿ç”¨ä¸¤é˜¶æ®µæäº¤æµç¨‹æ‰§è¡Œåˆ†ç±»åˆå¹¶æ“ä½œï¼Œä»¥è§£å†³é“¾å¼ä¾èµ–é—®é¢˜ã€‚
    """
    logger.info("--- [API] æ¥æ”¶åˆ°æ‰§è¡Œåˆ†ç±»åˆå¹¶è¯·æ±‚ (v2 - ä¸¤é˜¶æ®µ) ---")
    confirmed_merges = payload.get("confirmed_merges")
    if not isinstance(confirmed_merges, list):
        raise HTTPException(status_code=400, detail="è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦'confirmed_merges'åˆ—è¡¨ã€‚")

    if not confirmed_merges:
        return {"message": "æ²¡æœ‰éœ€è¦æ‰§è¡Œçš„åˆå¹¶æ“ä½œã€‚"}

    conn = metadata_db.get_db_connection()
    tasks_to_delete = set()
    successful_updates = 0
    try:
        # --- é˜¶æ®µä¸€ï¼šæ›´æ–°æ‰€æœ‰ä»å±å…³ç³» (åœ¨å•ä¸ªäº‹åŠ¡ä¸­å®Œæˆ) ---
        logger.info("--- [åˆå¹¶é˜¶æ®µ1/2] å¼€å§‹æ›´æ–°æ‰€æœ‰åˆ†ç±»çš„ä»å±å…³ç³» ---")
        with conn:
            for merge_item in confirmed_merges:
                from_cat = merge_item.get("from")
                to_cat = merge_item.get("to")
                if not from_cat or not to_cat: continue

                from_id, to_id = metadata_db.execute_category_merge(
                    from_domain_name=from_cat["domain"], from_task_name=from_cat["task"],
                    to_domain_name=to_cat["domain"], to_task_name=to_cat["task"],
                    conn=conn
                )
                if from_id is not None and from_id != to_id:
                    tasks_to_delete.add(from_id)
                    successful_updates += 1
        logger.info("--- [åˆå¹¶é˜¶æ®µ1/2] æ‰€æœ‰ä»å±å…³ç³»æ›´æ–°å®Œæ¯• ---")


        # --- é˜¶æ®µäºŒï¼šåˆ é™¤æ‰€æœ‰å†—ä½™çš„Task (åœ¨å¦ä¸€ä¸ªäº‹åŠ¡ä¸­å®Œæˆ) ---
        if tasks_to_delete:
            logger.info(f"--- [åˆå¹¶é˜¶æ®µ2/2] å¼€å§‹åˆ é™¤ {len(tasks_to_delete)} ä¸ªå†—ä½™ä»»åŠ¡ ---")
            with conn:
                for task_id in tasks_to_delete:
                    conn.execute("DELETE FROM tasks WHERE id = ?", (task_id,))
            logger.info("--- [åˆå¹¶é˜¶æ®µ2/2] æ‰€æœ‰å†—ä½™ä»»åŠ¡åˆ é™¤å®Œæ¯• ---")


        # --- æ–‡ä»¶å±‚é¢çš„æ¸…ç†å’Œå¯¹é½ (åœ¨æ‰€æœ‰æ•°æ®åº“æ“ä½œæˆåŠŸåè¿›è¡Œ) ---
        all_categories = ingestion_agent.get_known_categories()
        merge_map = { (item["from"]["domain"], item["from"]["task"]): (item["to"]["domain"], item["to"]["task"]) for item in confirmed_merges }

        # a. æ¸…ç† categories.json
        cats_to_delete_tuples = { (item["from"]["domain"], item["from"]["task"]) for item in confirmed_merges }
        temp_categories = all_categories.copy()
        for domain, task in cats_to_delete_tuples:
            if domain in temp_categories and task in temp_categories[domain]["tasks"]:
                del temp_categories[domain]["tasks"][task]
                if not temp_categories[domain]["tasks"]:
                     del temp_categories[domain]
        # vvv [ä¿®æ”¹] vvv
        with open(config_module.CATEGORIES_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump(temp_categories, f, indent=4, ensure_ascii=False)
        logger.info("categories.json æ–‡ä»¶å·²æ¸…ç†ã€‚")

        # b. å¯¹é½ user_preferences.json
        if config_module.USER_PREFERENCES_PATH.exists():
            with open(config_module.USER_PREFERENCES_PATH, 'r', encoding='utf-8') as f:
                user_prefs = json.load(f)

            original_prefs = user_prefs.get("selected_categories", [])
            updated_prefs = [
                {"domain": merge_map.get((p["domain"], p["task"]), (p["domain"], p["task"]))[0],
                 "task": merge_map.get((p["domain"], p["task"]), (p["domain"], p["task"]))[1]}
                for p in original_prefs
            ]
            unique_prefs_tuples = sorted(list(set((d['domain'], d['task']) for d in updated_prefs)))
            user_prefs["selected_categories"] = [{"domain": d, "task": t} for d, t in unique_prefs_tuples]

            with open(config_module.USER_PREFERENCES_PATH, 'w', encoding='utf-8') as f:
                json.dump(user_prefs, f, indent=4, ensure_ascii=False)
            logger.info("user_preferences.json æ–‡ä»¶å·²å¯¹é½ã€‚")
        # ^^^ [ä¿®æ”¹] ^^^

        message = f"åˆ†ç±»æ¸…ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_updates} / {len(confirmed_merges)} æ¡åˆå¹¶è§„åˆ™ã€‚"
        logger.info(message)
        return {"message": message}

    except Exception as e:
        logger.critical(f"æ‰§è¡Œåˆå¹¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ã€‚")
    finally:
        if conn:
            conn.close()