🚀 Auto-ARVIX 智能科研助手 (v1.0)
Auto-ARVIX 是一款先进的、自动化的科研论文处理与智能问答系统。项目 v1.0 版本在原有基础上引入了 重排（Reranking）模型 与 动态 LLM 切换 功能，旨在通过结合最前沿的 AI 技术栈，从根本上改变研究人员与海量 arXiv 论文的交互方式。

系统能够自动跟踪 arXiv 上的最新论文，进行深度解析、智能分类和高质量摘要，并将其构建成一个结构化的、持续增长的本地知识库。用户可以通过一个清爽的 Web 界面，使用自然语言与知识库交互，或动态调整系统的核心参数，以适应不同的研究需求。

✨ v1.0 核心升级
检索精度革命：引入 Reranker 模型
在传统的向量检索（Recall）之后，新增了一个 重排（Rerank） 阶段。
使用 Qwen3-Reranker-0.6B 模型对召回的候选文档进行二次排序，确保最终用于生成答案的上下文与用户问题的相关性达到最高，显著提升了回答的精准度。
灵活性与可扩展性：动态 LLM 切换
系统不再绑定单一的大语言模型。用户现在可以在 Web UI 上，从多个已配置的本地 Ollama 模型（如 gemma，qwen3）中自由选择，以执行不同的问答或分析任务。
为不同的任务场景（如要求速度、要求代码能力、要求逻辑推理）选择最合适的模型提供了可能。
完全的 UI 参数化控制
将项目的核心行为参数，如“每日论文处理上限”、“PDF 解析策略”、“检索候选数”等，全部开放到 Web UI 的“系统设置”页面。
用户无需修改任何代码，即可通过图形化界面实时调整系统配置，极大增强了易用性和灵活性。

核心功能
🤖 全自动每日工作流
自动获取: 每日自动从 arXiv 获取用户指定领域（如 cs.AI, cs.CV）的最新论文。
健壮解析: 具备高精度（MonkeyOCR）和高速度（Unstructured）两种可切换的 PDF 解析策略。
持续学习: 自动将新论文处理并添加到本地知识库中，实现知识的持续增长。

🧠 智能化论文入库 (Ingestion)
AI 驱动分类: 利用 LLM 对每篇论文进行内容分析，将其自动分类到精确的“领域-任务”层次结构中，并能通过 AI 自动提出和执行分类合并建议，保持体系整洁。
高质量 AI 摘要: 采用 Map-Reduce 策略，对长篇论文进行分块处理和多轮摘要，生成远超原文摘要质量的深度技术摘要。
分层知识库: 将论文的元数据存储在 SQLite 数据库中，同时将文本块的向量嵌入存储在为 GPU 优化的 FAISS 索引中，实现高效的混合检索。

💡 自适应问答流程 (Query)
召回-重排-生成: 采用先进的 Recall -> Rerank -> Generate 流程。首先通过向量数据库快速召回大量相关文档，然后通过 Reranker 模型精选出最核心的上下文，最后交由 LLM 生成答案。
本地优先，在线增强: 系统首先在本地知识库中检索。如果信息不足，规划代理会自动启动在线搜索，获取最新的信息，并将其无缝整合到回答中。
可追溯的答案: 所有回答都基于从多篇最相关论文中提炼的信息，并提供完整的来源论文列表和 PDF 链接，方便用户验证和深入阅读。

🖥️ 简洁的 Web 界面
提供一个直观的前端界面，可以进行智能问答、查看历史报告、一键触发每日工作流，以及动态配置所有核心参数。
问答过程采用流式响应（Server-Sent Events），实时展示 AI 的处理进度，提升用户体验。

🏗️ 系统架构
项目采用现代化、模块化的 Python 后端架构，由 FastAPI 和 Typer 驱动。v1.0 版本在 H-RAG 系统中增加了 Reranker 模块。

+---------------------+      +-------------------------+      +----------------------+
|     Web UI (Vue.js) |----->|   FastAPI Backend API   |<---->|   CLI (Typer)        |
|    (web/index.html) |      |    (api/main.py)        |      |   (main.py)          |
+---------------------+      +-------------------------+      +----------------------+
                                       |
                                       | [Workflows]
                                       | (daily_flow, query_flow)
                                       |
           +------------------------------------------------------------------+
           |                           Core Services                          |
           +------------------------------------------------------------------+
           |                                                                  |
+----------v----------+    +-------------------------+    +-------------------------+
|     AI Agents       |    |     H-RAG System        |    |   Data Ingestion        |
| (ingestion, query)  |    |    (hrag_manager)       |    | (arxiv_fetcher,         |
+---------------------+    +-------------------------+    |  pdf_processor)         |
| - Classification    |    | - Metadata DB (SQLite)  |    +-------------------------+
| - Summarization     |    | - Vector DB (FAISS)     |
| - Planning & Search |    | - Embedding Engine      |
+---------------------+    | - Reranker              |  <-- (新增)
           |               +-------------------------+
           |                          ^
           |                          |
+----------v--------------------------+
|      LLM Client (Ollama)           |
+------------------------------------+

🛠️ 安装与配置
在开始之前，请确保您的系统已安装以下依赖：

Conda: 用于管理 Python 环境和核心依赖。

NVIDIA 驱动 & CUDA: 为了使用 GPU 加速 FAISS 和 PyTorch。

Ollama: 用于在本地运行大语言模型。

LaTeX 发行版 (如 TeX Live, MiKTeX): 用于生成 PDF 报告。

1. 克隆项目仓库
git clone <your-repo-url>
cd <your-repo-directory>

2. 安装依赖
项目提供了一个便捷的安装脚本。它会自动创建 Conda 环境（环境名为 auto_arvix）并安装所有必需的库，包括 GPU 版本的 Pytorch 和 FAISS。

./run.sh install

3. 下载并运行 Ollama 大模型
本项目默认配置了两个 LLM 和一个 Reranker。请运行以下命令来拉取所需模型。

注意: Embedding 和 Reranker 模型会在首次运行时由 transformers 库自动下载，您只需确保网络连接通畅。

# 拉取核心 LLM (请确保这两个模型已在本地Ollama中)
ollama pull auto-arvix-unsloth-pro
ollama pull qwen3:8b-q8_0

确保 Ollama 服务正在后台运行。

4. （可选）验证 GPU 环境
您可以运行测试脚本来确保 PyTorch 和 FAISS 能够正确利用您的 GPU。

./run.sh test-gpu

🚀 运行项目
您可以通过 run.sh 脚本以两种模式运行本项目。

启动 Web 服务器（推荐）
此命令会启动 FastAPI Web 服务。您可以随时通过浏览器访问 Web 界面，并从 UI 触发每日任务。

./run.sh server

服务启动后，请在浏览器中访问 http://0.0.0.0:5002 (或您配置的主机和端口)。

仅手动执行每日任务
如果您想通过定时任务（如 Cron Job）来独立运行每日的论文获取和处理流程，而不启动 Web 服务，请使用此命令。

./run.sh daily

⚙️ 通过 UI 进行配置
v1.0 版本的一大亮点就是强大的 UI 配置能力。进入 Web 界面的 “系统设置” 页面，您可以轻松调整：

全局运行参数: 控制每日处理论文的数量、PDF 解析的精度与速度、检索算法的深度等。

用户偏好: 通过勾选您感兴趣的“领域-任务”组合，精确地告诉系统为您筛选和处理哪些论文。

所有设置都会实时保存，并在下一次任务运行时自动生效，无需重启服务或修改任何代码。


📂 项目结构
.
├── agents/              # 专用AI代理逻辑
├── api/                 # FastAPI 路由与数据模型
├── core/                # 核心模块: 配置, 日志, LLM客户端, 启动引导
├── data_ingestion/      # 数据获取与处理 (arXiv, PDF)
├── hrag/                # H-RAG核心: DB, 向量, 嵌入, Reranker(新增)
│   └── reranker.py      # <-- 新增 Reranker 模块
├── storage/             # 默认数据存储目录 (自动创建)
├── utils/               # 工具脚本 (PDF报告生成)
├── web/                 # Web前端界面 (HTML, Vue.js)
├── workflows/           # 核心业务流程编排
├── main.py              # 命令行接口 (Typer) 入口
├── run.sh               # 环境管理与启动脚本
└── requirements.txt     # Python依赖列表
