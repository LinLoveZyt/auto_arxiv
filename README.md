README.md (v1.0.6)
🚀 Auto-ARXIV 智能科研助手 (v1.0.6)

Auto-ARXIV 是一款先进的、自动化的科研论文处理与智能问答系统。项目 v1.0.6 版本完成了一次核心流程的重大升级，引入了基于OCR的后置质量筛选机制，从根本上解决了依赖不可靠元数据的问题，确保了只有真正高质量、高相关性的论文才能进入您的知识库。

系统能够自动跟踪 arXiv 上的最新论文，进行深度解析和智能分类。最关键的是，它现在采用先进的“两阶段筛选”策略：首先通过兴趣偏好和研究计划进行无上限的广泛筛选，然后对通过初筛的论文逐一进行高精度的PDF内容解析，最后利用从原文中提取出的精确作者与机构信息，进行严格的“强团队/强作者”质量把关。

✨ v1.0.6 核心升级：基于原文的精准质量筛选

v1.0.6 版本是一次关于“事实依据”的架构革命。我们不再信任 arXiv API 返回的、时常缺失或不规范的作者机构元数据，而是将质量把关的环节，后置到高精度PDF解析（MonkeyOCR）完成之后，确保每一次决策都基于从论文原文中提取出的可靠信息。

核心流程再造：两阶段筛选 (Two-Stage Filtering)

兴趣初筛 (Interest Filtering):

目标: 广度。此阶段的目标是“不错过任何一篇可能相关的论文”。

执行: 系统会像以前一样，根据您设置的固定分类偏好和临时的“动态调研计划”，对arXiv上的海量论文进行第一轮筛选。此阶段没有数量上限，力求全面。

质量精筛 (Quality Filtering):

目标: 精度。此阶段的目标是“只保留真正有价值的论文”。

执行:

逐一解析: 通过兴趣初筛的论文会进入一个待定队列，系统将逐篇下载其PDF并使用MonkeyOCR进行深度解析。

信息提取: 一个全新的AI Agent会从OCR生成的结构化文本中，智能提取出精确的作者列表和所属机构列表。

可靠决策: “强团队”和“强作者”的判断，现在完全基于这份从原文提取的、可靠的信息进行。只有通过了这道质量关卡的论文，才会被最终采纳。

资源清理: 对于未通过质量筛选的论文，其下载的PDF和生成的OCR文件会被自动删除，有效节约了存储空间。

这一架构性的变革带来了三大优势:

准确性: 彻底摆脱了对arXiv API不可靠元数据的依赖，筛选决策更加精准。

鲁棒性: 即使论文提交者没有在arXiv上填写机构信息，只要PDF原文中有，系统就能正确识别。

高效性: 昂贵的OCR操作只对兴趣相关的论文执行，避免了对大量无关论文的无效处理。

核心功能
🤖 全自动每日工作流

智能获取: (v1.0.5 升级) 每日从 arXiv 按照您自定义的日期范围，通过“无限滚动”机制获取所有相关论文。

两阶段筛选: (v1.0.6 核心升级)

兴趣初筛: 基于用户偏好和动态研究计划，无上限筛选所有可能相关的论文。

质量精筛: 对初筛通过的论文进行逐一OCR，并基于从原文提取的精确作者/机构信息，进行“强团队/强作者”的最终把关。

健壮解析: 具备高精度（MonkeyOCR）和高速度（Unstructured）两种可切换的 PDF 解析策略。

持续学习: 自动将最终通过筛选的顶级论文处理并添加到本地知识库中，实现高质量知识的持续增长。

🧠 智能化论文入库 (Ingestion)

高级AI驱动分类: 利用先进的两阶段RAG辅助决策流程，对每篇论文进行高精度、高一致性的分类。

原文信息提取: (v1.0.6 新增) 专门的AI Agent负责从OCR结果中精准提取作者和机构信息。

高质量AI摘要: 采用 Map-Reduce 策略，对长篇论文生成远超原文摘要质量的深度技术摘要。

分层知识库: 将论文的元数据存储在 SQLite 数据库中，同时将文本块的向量嵌入存储在为GPU优化的 FAISS 索引中。

💡 自适应问答流程 (Query)

召回-重排-生成: 采用先进的 Recall -> Rerank -> Generate 流程，从本地高质量知识库中生成精准、可追溯的答案。

本地优先，在线增强: 在信息不足时，可启动在线搜索，获取最新的信息并无缝整合到回答中。

可追溯的答案: 所有回答都提供完整的来源论文列表和 PDF 链接，方便用户验证和深入阅读。

🖥️ 简洁的 Web 界面

(v1.0.6 升级) 提供一个直观的前端界面，可以进行智能问答、查看历史报告、一键触发每日工作流，并动态配置所有核心参数，包括新增的“强团队”和“强作者”名单。

系统架构
v1.0.6 版本的核心改进在于 Workflows 和 Agents 模块的重构，实现了将Data Ingestion中的OCR步骤，作为两阶段筛选的中间环节，从而彻底改变了数据流和决策依据。

+---------------------+      +-------------------------+      +----------------------+
|  Web UI (Vue.js)    |----->|   FastAPI Backend API   |<---->|   CLI (Typer)        |
| (Quality List Mgmt) |      |    (api/main.py)        |      |   (main.py)          |
+---------------------+      +-------------------------+      +----------------------+
                                       |
                                       | [Workflows] (Two-Stage Filtering Logic)
                                       | (daily_flow.py)
                                       |
           +------------------------------------------------------------------+
           |                           Core Services                          |
           +------------------------------------------------------------------+
           |                                                                  |
+----------v----------+    +-------------------------+    +-------------------------+
|     AI Agents       |    |     H-RAG System        |    |   Data Ingestion        |
| (OCR-based Quality) |    |    (hrag_manager)       |    | (Initial Fetch & OCR)   |
+---------------------+    +-------------------------+    +-------------------------+
| - Classification    |    | - Metadata DB (SQLite)  |    | - arxiv_fetcher         |
| - Summarization     |    | - Vector DB (FAISS)     |    | - pdf_processor         |
| - Authorship Extract|    | - Embedding Engine      |    +-------------------------+
+---------------------+    | - Reranker              |
           ^               +-------------------------+
           | (RAG Context)            ^
           |                          |
+----------v--------------------------+
|      LLM Client (Ollama)           |
+------------------------------------+

🛠️ 安装与配置
在开始之前，请确保您的系统已安装以下依赖：

Conda: 用于管理 Python 环境和核心依赖。

NVIDIA 驱动 & CUDA: 为了使用 GPU 加速 FAISS 和 PyTorch。

Ollama: 用于在本地运行大语言模型。

LaTeX 发行版 (如 TeX Live, MiKTeX): 用于生成 PDF 报告。

1. 克隆项目仓库

Bash

git clone <your-repo-url>
cd <your-repo-directory>
2. 安装依赖
项目提供了一个便捷的安装脚本。它会自动创建 Conda 环境（环境名为 auto_arxiv）并安装所有必需的库。

Bash

./run.sh install
3. 下载并运行 Ollama 大模型
请确保您已在本地 Ollama 中拉取了项目所需的模型。Embedding 和 Reranker 模型会在首次运行时由 transformers 库自动下载。

Bash

# 拉取核心LLM (示例)
ollama pull auto-arxiv-unsloth-pro
ollama pull qwen2:7b-instruct-q8_0
确保 Ollama 服务正在后台运行。

4. （可选）验证 GPU 环境

Bash

./run.sh test-gpu
🚀 运行项目
您可以通过 run.sh 脚本以两种模式运行本项目。

启动 Web 服务器（推荐）
此命令会启动 FastAPI Web 服务。您可以随时通过浏览器访问 Web 界面，并从 UI 触发每日任务。

Bash

./run.sh server
服务启动后，请在浏览器中访问 http://0.0.0.0:5002 (或您配置的主机和端口)。

仅手动执行每日任务
如果您想通过定时任务（如 Cron Job）来独立运行每日的论文获取和处理流程，而不启动 Web 服务，请使用此命令。

Bash

./run.sh daily
通过 UI 进行配置
v1.0.6 版本继承并增强了强大的 UI 配置能力。进入 Web 界面的 “系统设置” 页面，您可以轻松调整：

全局运行参数: 控制每日处理论文的数量、PDF 解析的精度与速度等。

用户偏好: 精确地告诉系统为您筛选哪些领域的论文。

质量筛选配置: (v1.0.6 新增) 在UI界面直接管理“强团队”和“强作者”的名单，实时更新您的质量标准。

📂 项目结构
.
├── agents/              # 专用AI代理逻辑 (新增了从OCR提取信息的Agent)
├── api/                 # FastAPI 路由与数据模型 (新增了质量名单管理接口)
├── core/                # 核心模块: 配置, 日志, LLM客户端, 启动引导
├── data_ingestion/      # 数据获取与处理
├── hrag/                # H-RAG核心: DB, 向量, 嵌入, Reranker
├── storage/             # 默认数据存储目录 (新增strong_teams/authors.json)
├── utils/               # 工具脚本 (PDF报告生成逻辑已升级支持目录)
├── web/                 # Web前端界面 (新增质量名单管理功能)
├── workflows/           # 核心业务流程编排 (每日工作流已重构为两阶段筛选)
├── main.py              # 命令行接口 (Typer) 入口
├── test_quality_filter.py # (新增) 独立的质量筛选机制测试脚本
├── run.sh               # 环境管理与启动脚本
└── requirements.txt     # Python依赖列表