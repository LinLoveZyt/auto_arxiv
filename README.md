🚀 Auto-ARVIX 智能科研助手 (v1.0.4)
Auto-ARVIX 是一款先进的、自动化的科研论文处理与智能问答系统。项目 v1.0.4 版本 聚焦于系统的核心——知识的构建与管理，引入了革命性的高质量分类流程和数据库驱动的一致性架构，从根本上解决了知识库的准确性与可维护性问题。

系统能够自动跟踪 arXiv 上的最新论文，进行深度解析和智能分类，并将其构建成一个结构化的、持续增长的本地知识库。用户可以通过一个清爽的 Web 界面，使用自然语言与知识库交互，或动态调整系统的核心参数，以适应不同的研究需求。

✨ v1.0.4 核心升级：分类质量与一致性革命
v1.0.4 版本是对项目核心的一次深度重构，旨在构建一个更智能、更健壮、更一致的知识体系。

1. 新一代高质量分类流程：RAG辅助决策
我们彻底重塑了论文的分类方式，引入了先进的 “独立建议 + RAG辅助决策” 两阶段流程，取代了以往单一的分类方法。

智能候选: 系统首先像以前一样，对新论文的标题和摘要进行分析，得出一个独立的“候选分类”。

分层上下文检索: 关键的创新在于，系统会利用这个“候选分类”，通过H-RAG的分层特性，高效地从数据库中检索出同领域、同任务下，内容最相似的几篇已有论文。

AI专家决策: 一个经过精心设计的全新AI Agent会扮演“分类学家”的角色。它会综合评估新论文、独立建议，以及从已有论文中检索到的丰富上下文（包括它们的分类和相似度分数），最终做出专家级的判断——是采纳已有分类以保持一致，还是验证并创建真正的新分类。

这个新流程在保证对新概念的开放性的同时，极大地减少了冗余分类的产生，确保了分类体系的长期健康。

2. 架构基石：数据库作为唯一事实来源
为了彻底解决数据不一致的问题，我们将 SQLite数据库确立为分类体系的“唯一事实来源”。

现在，所有的分类（包括那些暂时没有论文的“空分类”）都统一在数据库中进行管理。

前端UI展示所需的 categories.json 文件，现在完全由一个工具函数从数据库中自动导出生成。

无论是每日任务、类别收集，还是分类合并，任何对分类体系的修改都会首先作用于数据库，然后自动同步到JSON文件，确保了系统状态的绝对一致。

3. 新增特性：轻量级“元数据-Only”入库
为了更快速地扩充和学习分类体系，我们引入了轻量级入库模式。

在“类别收集”工作流中，系统现在可以只将论文的元数据（标题、摘要、作者等）和其高质量的分类结果存入数据库，而无需下载和解析完整的PDF。

这使得用户可以以极低的成本，快速地让系统学习大量论文的分类知识，从而迅速构建起一个庞大而准确的分类目录。

核心功能
🤖 全自动每日工作流
自动获取: 每日自动从 arXiv 获取用户指定领域（如 cs.AI, cs.CV）的最新论文。

健壮解析: 具备高精度（MonkeyOCR）和高速度（Unstructured）两种可切换的 PDF 解析策略。

持续学习: 自动将新论文处理并添加到本地知识库中，实现知识的持续增长。

🧠 智能化论文入库 (Ingestion)
高级AI驱动分类: (v1.0.4 升级) 利用全新的两阶段RAG辅助决策流程，对每篇论文进行高精度、高一致性的分类，并能通过AI自动提出和执行分类合并建议，保持体系整洁。

轻量级元数据入库: (v1.0.4 新增) 支持只入库论文元数据和分类信息，用于快速扩充系统的分类知识。

高质量AI摘要: 采用 Map-Reduce 策略，对长篇论文进行分块处理和多轮摘要，生成远超原文摘要质量的深度技术摘要。

分层知识库: 将论文的元数据存储在 SQLite 数据库中（作为唯一事实来源），同时将文本块的向量嵌入存储在为GPU优化的 FAISS 索引中，实现高效的混合检索。

💡 自适应问答流程 (Query)
召回-重排-生成: 采用先进的 Recall -> Rerank -> Generate 流程。首先通过向量数据库快速召回大量相关文档，然后通过 Reranker 模型精选出最核心的上下文，最后交由 LLM 生成答案。

本地优先，在线增强: 系统首先在本地知识库中检索。如果信息不足，规划代理会自动启动在线搜索，获取最新的信息，并将其无缝整合到回答中。

可追溯的答案: 所有回答都基于从多篇最相关论文中提炼的信息，并提供完整的来源论文列表和 PDF 链接，方便用户验证和深入阅读。

🖥️ 简洁的 Web 界面
提供一个直观的前端界面，可以进行智能问答、查看历史报告、一键触发每日工作流，以及动态配置所有核心参数。

问答过程采用流式响应（Server-Sent Events），实时展示 AI 的处理进度，提升用户体验。

🏗️ 系统架构
项目采用现代化、模块化的 Python 后端架构，由 FastAPI 和 Typer 驱动。v1.0.4 版本的核心改进在于强化了 Core Services 中 H-RAG System 与 AI Agents 之间的交互逻辑，并确立了数据库的中心地位。

+---------------------+      +-------------------------+      +----------------------+
|     Web UI (Vue.js) |----->|   FastAPI Backend API   |<---->|   CLI (Typer)        |
|    (web/index.html) |      |    (api/main.py)        |      |   (main.py)          |
+---------------------+      +-------------------------+      +----------------------+
                                       |
                                       | [Workflows]
                                       | (daily_flow, query_flow)
                                       |
           +------------------------------------------------------------------+
           |                           Core Services                          |
           +------------------------------------------------------------------+
           |                                                                  |
+----------v----------+    +-------------------------+    +-------------------------+
|     AI Agents       |    |     H-RAG System        |    |   Data Ingestion        |
| (ingestion, query)  |    |    (hrag_manager)       |    | (arxiv_fetcher,         |
+---------------------+    +-------------------------+    |  pdf_processor)         |
| - Classification    |    | - Metadata DB (SQLite)  |    +-------------------------+
| - Summarization     |    |   (Single Source of Truth)
| - Planning & Search |    | - Vector DB (FAISS)     |
+---------------------+    | - Embedding Engine      |
           ^               | - Reranker              |
           |               +-------------------------+
           | (RAG Context)            ^
           |                          |
+----------v--------------------------+
|      LLM Client (Ollama)           |
+------------------------------------+
🛠️ 安装与配置
在开始之前，请确保您的系统已安装以下依赖：

Conda: 用于管理 Python 环境和核心依赖。

NVIDIA 驱动 & CUDA: 为了使用 GPU 加速 FAISS 和 PyTorch。

Ollama: 用于在本地运行大语言模型。

LaTeX 发行版 (如 TeX Live, MiKTeX): 用于生成 PDF 报告。

1. 克隆项目仓库
Bash

git clone <your-repo-url>
cd <your-repo-directory>
2. 安装依赖
项目提供了一个便捷的安装脚本。它会自动创建 Conda 环境（环境名为 auto_arvix）并安装所有必需的库。

Bash

./run.sh install
3. 下载并运行 Ollama 大模型
请确保您已在本地 Ollama 中拉取了项目所需的模型。Embedding 和 Reranker 模型会在首次运行时由 transformers 库自动下载。

Bash

# 拉取核心LLM (示例)
ollama pull auto-arvix-unsloth-pro
ollama pull qwen3:8b-q8_0
确保 Ollama 服务正在后台运行。

4. （可选）验证 GPU 环境
Bash

./run.sh test-gpu
🚀 运行项目
您可以通过 run.sh 脚本以两种模式运行本项目。

启动 Web 服务器（推荐）
此命令会启动 FastAPI Web 服务。您可以随时通过浏览器访问 Web 界面，并从 UI 触发每日任务。

Bash

./run.sh server
服务启动后，请在浏览器中访问 http://0.0.0.0:5002 (或您配置的主机和端口)。

仅手动执行每日任务
如果您想通过定时任务（如 Cron Job）来独立运行每日的论文获取和处理流程，而不启动 Web 服务，请使用此命令。

Bash

./run.sh daily
⚙️ 通过 UI 进行配置
v1.0.4 版本继承了强大的 UI 配置能力。进入 Web 界面的 “系统设置” 页面，您可以轻松调整：

全局运行参数: 控制每日处理论文的数量、PDF 解析的精度与速度、检索算法的深度等。

用户偏好: 通过勾选您感兴趣的“领域-任务”组合，精确地告诉系统为您筛选和处理哪些论文。

所有设置都会实时保存，并在下一次任务运行时自动生效，无需重启服务或修改任何代码。

📂 项目结构
.
├── agents/              # 专用AI代理逻辑 (分类逻辑已升级)
├── api/                 # FastAPI 路由与数据模型
├── core/                # 核心模块: 配置, 日志, LLM客户端, 启动引导
├── data_ingestion/      # 数据获取与处理 (arXiv, PDF)
├── hrag/                # H-RAG核心: DB, 向量, 嵌入, Reranker
├── storage/             # 默认数据存储目录 (自动创建)
├── utils/               # 工具脚本 (PDF报告生成)
├── web/                 # Web前端界面 (HTML, Vue.js)
├── workflows/           # 核心业务流程编排 (流程已升级)
├── main.py              # 命令行接口 (Typer) 入口
├── run.sh               # 环境管理与启动脚本
└── requirements.txt     # Python依赖列表