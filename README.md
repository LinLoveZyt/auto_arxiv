README.md (v1.0.5)
🚀 Auto-ARXIV 智能科研助手 (v1.0.5)
Auto-ARXIV 是一款先进的、自动化的科研论文处理与智能问答系统。项目 v1.0.5 版本聚焦于将系统从一个“自动化工具”升级为真正的“智能研究助理”，通过引入动态调研计划、无限滚动式的论文获取机制和更具洞察力的报告系统，极大地提升了科研的灵活性与深度。

系统能够自动跟踪 arXiv 上的最新论文，进行深度解析和智能分类，并将其构建成一个结构化的、持续增长的本地知识库。用户可以通过一个清爽的 Web 界面，使用自然语言与知识库交互，或通过全新的交互式面板，动态指导每日的论文筛选和获取流程。

✨ v1.0.5 核心升级：动态调研与智能获取
v1.0.5 版本是一次交互与智能的革命，旨在让系统更懂您的临时需求，获取信息更全，结果更透明。

新增特性：动态调研计划 (Dynamic Research Plan)
我们为每日任务引入了一个全新的交互式启动面板。现在，您在启动每日工作流之前，可以输入一段临时的、具体的“调研计划”。系统将采用先进的二级筛选策略：

固定偏好筛选: 首先，系统会像以前一样，根据您在设置中保存的固定分类偏好进行第一轮筛选。

AI助理二次评估: 对于未通过第一轮筛选的论文，一个专门的AI Agent会介入，将论文内容与您的“动态调研计划”进行深度比对，判断其潜在价值和相关性。

这个功能让系统超越了僵硬的规则，能够捕捉到您临时的、跨领域的、或更具体的隐性研究兴趣，不错过任何一篇高价值论文。

获取机制革命：“无限滚动”式获取与智能终止
我们彻底解决了旧版本中论文获取数量不足或过多的问题。新的获取机制：

无限获取: 系统不再局限于获取固定的100或500篇论文。它会持续不断地从 arXiv 服务获取指定日期范围内的所有新论文，直到遍历完所有结果。

智能终止: 在获取过程中，系统会实时监控已筛选出的论文数量。一旦达到您设定的“每日处理上限”，获取和筛选流程将立即终止，从而避免了不必要的API请求和算力消耗，在保证“找够”的同时做到了“不浪费”。

交互升级：可自定义的获取周期
每日任务的启动面板中，除了调研计划，还新增了日期范围选择器。您可以自由指定想要检索的论文发布时间区间，无论是想回顾上周的进展，还是只看昨天的最新发表，都只需轻松一点。系统默认依然是最近3天。

报告系统增强：可追溯的筛选理由
为了让每一份报告都更具洞察力，我们增强了报告的透明度。如果一篇论文是基于您的“动态调研计划”被AI助理选中的，那么AI助理给出的详细筛选理由（例如：“该论文的XX方法与您调研计划中的YY问题高度相关…”）将会被自动、清晰地呈现在该篇论文的PDF报告章节中，让您一目了然为何收录此文。

核心功能
🤖 全自动每日工作流

智能获取: (v1.0.5 升级) 每日从 arXiv 按照您自定义的日期范围，通过“无限滚动”机制获取所有相关论文，直到满足处理上限或遍历完所有结果。

健壮解析: 具备高精度（MonkeyOCR）和高速度（Unstructured）两种可切换的 PDF 解析策略。

持续学习: 自动将新论文处理并添加到本地知识库中，实现知识的持续增长。

🧠 智能化论文入库 (Ingestion)

高级AI驱动分类: 利用先进的两阶段RAG辅助决策流程，对每篇论文进行高精度、高一致性的分类，并能通过AI自动提出和执行分类合并建议。

动态二次筛选: (v1.0.5 新增) 在固定分类偏好之外，利用AI助理根据用户的临时“调研计划”进行二次评估和筛选。

高质量AI摘要: 采用 Map-Reduce 策略，对长篇论文进行分块处理和多轮摘要，生成远超原文摘要质量的深度技术摘要。

分层知识库: 将论文的元数据存储在 SQLite 数据库中，同时将文本块的向量嵌入存储在为GPU优化的 FAISS 索引中，实现高效的混合检索。

💡 自适应问答流程 (Query)

召回-重排-生成: 采用先进的 Recall -> Rerank -> Generate 流程。首先通过向量数据库快速召回大量相关文档，然后通过 Reranker 模型精选出最核心的上下文，最后交由 LLM 生成答案。

本地优先，在线增强: 系统首先在本地知识库中检索。如果信息不足，规划代理会自动启动在线搜索，获取最新的信息，并将其无缝整合到回答中。

可追溯的答案: 所有回答都基于从多篇最相关论文中提炼的信息，并提供完整的来源论文列表和 PDF 链接，方便用户验证和深入阅读。

🖥️ 简洁的 Web 界面

(v1.0.5 升级) 提供一个直观的前端界面，可以进行智能问答、查看历史报告、通过带有“调研计划”和“日期选择”的交互式面板一键触发每日工作流，以及动态配置所有核心参数。

问答过程采用流式响应（Server-Sent Events），实时展示 AI 的处理进度，提升用户体验。

🏗️ 系统架构
项目采用现代化、模块化的 Python 后端架构，由 FastAPI 和 Typer 驱动。v1.0.5 版本的核心改进在于通过引入动态参数和生成器机制，极大地强化了 Workflows 与 Data Ingestion 模块的交互灵活性，并通过增强的 AI Agents 为筛选和报告流程注入了更高阶的智能。

+---------------------+      +-------------------------+      +----------------------+
|  Web UI (Vue.js)    |----->|   FastAPI Backend API   |<---->|   CLI (Typer)        |
| (Interactive Panel) |      |    (api/main.py)        |      |   (main.py)          |
+---------------------+      +-------------------------+      +----------------------+
                                       |
                                       | [Workflows] (Enhanced Logic)
                                       | (daily_flow, query_flow)
                                       |
           +------------------------------------------------------------------+
           |                           Core Services                          |
           +------------------------------------------------------------------+
           |                                                                  |
+----------v----------+    +-------------------------+    +-------------------------+
|     AI Agents       |    |     H-RAG System        |    |   Data Ingestion        |
| (Dynamic Evaluation)|    |    (hrag_manager)       |    | (Generator-based Fetch) |
+---------------------+    +-------------------------+    +-------------------------+
| - Classification    |    | - Metadata DB (SQLite)  |    | - arxiv_fetcher         |
| - Summarization     |    | - Vector DB (FAISS)     |    | - pdf_processor         |
| - Reporting (New)   |    | - Embedding Engine      |    +-------------------------+
+---------------------+    | - Reranker              |
           ^               +-------------------------+
           | (RAG Context)            ^
           |                          |
+----------v--------------------------+
|      LLM Client (Ollama)           |
+------------------------------------+
🛠️ 安装与配置
在开始之前，请确保您的系统已安装以下依赖：

Conda: 用于管理 Python 环境和核心依赖。

NVIDIA 驱动 & CUDA: 为了使用 GPU 加速 FAISS 和 PyTorch。

Ollama: 用于在本地运行大语言模型。

LaTeX 发行版 (如 TeX Live, MiKTeX): 用于生成 PDF 报告。

1. 克隆项目仓库

Bash

git clone <your-repo-url>
cd <your-repo-directory>
2. 安装依赖
项目提供了一个便捷的安装脚本。它会自动创建 Conda 环境（环境名为 auto_arxiv）并安装所有必需的库。

Bash

./run.sh install
3. 下载并运行 Ollama 大模型
请确保您已在本地 Ollama 中拉取了项目所需的模型。Embedding 和 Reranker 模型会在首次运行时由 transformers 库自动下载。

Bash

# 拉取核心LLM (示例)
ollama pull auto-arxiv-unsloth-pro
ollama pull qwen2:7b-instruct-q8_0
确保 Ollama 服务正在后台运行。

4. （可选）验证 GPU 环境

Bash

./run.sh test-gpu
🚀 运行项目
您可以通过 run.sh 脚本以两种模式运行本项目。

启动 Web 服务器（推荐）
此命令会启动 FastAPI Web 服务。您可以随时通过浏览器访问 Web 界面，并从 UI 触发每日任务。

Bash

./run.sh server
服务启动后，请在浏览器中访问 http://0.0.0.0:5002 (或您配置的主机和端口)。

仅手动执行每日任务
如果您想通过定时任务（如 Cron Job）来独立运行每日的论文获取和处理流程，而不启动 Web 服务，请使用此命令。

Bash

./run.sh daily
⚙️ 通过 UI 进行配置
v1.0.5 版本继承并增强了强大的 UI 配置能力。进入 Web 界面的 “系统设置” 页面，您可以轻松调整：

全局运行参数: 控制每日处理论文的数量、PDF 解析的精度与速度、检索算法的深度等。

用户偏好: 通过勾选您感兴趣的“领域-任务”组合，精确地告诉系统为您筛选和处理哪些论文。

所有设置都会实时保存，并在下一次任务运行时自动生效，无需重启服务或修改任何代码。

📂 项目结构
.
├── agents/              # 专用AI代理逻辑 (筛选和报告逻辑已升级)
├── api/                 # FastAPI 路由与数据模型 (接口已升级)
├── core/                # 核心模块: 配置, 日志, LLM客户端, 启动引导
├── data_ingestion/      # 数据获取与处理 (获取逻辑已升级)
├── hrag/                # H-RAG核心: DB, 向量, 嵌入, Reranker
├── storage/             # 默认数据存储目录 (自动创建)
├── utils/               # 工具脚本 (PDF报告生成逻辑已升级)
├── web/                 # Web前端界面 (交互面板已升级)
├── workflows/           # 核心业务流程编排 (每日工作流已升级)
├── main.py              # 命令行接口 (Typer) 入口
├── run.sh               # 环境管理与启动脚本
└── requirements.txt     # Python依赖列表