# workflows/daily_flow.py
import random
import logging
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any

from core import config as config_module
from data_ingestion import arxiv_fetcher
from hrag import metadata_db, vector_db
from workflows.ingestion_flow import process_papers_list
from agents import report_agent, ingestion_agent
from utils import pdf_generator
import arxiv

logger = logging.getLogger(__name__)


def _get_user_preferences() -> List[Dict[str, str]]:
    """ä»JSONæ–‡ä»¶åŠ è½½ç”¨æˆ·é€‰æ‹©çš„åå¥½ç±»åˆ«ã€‚"""
    logger.info(f"æ­£åœ¨æ£€æŸ¥ç”¨æˆ·åå¥½æ–‡ä»¶: {config_module.USER_PREFERENCES_PATH}")
    
    if not config_module.USER_PREFERENCES_PATH.exists():
        logger.warning("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶ user_preferences.json ä¸å­˜åœ¨ã€‚")
        return []
        
    try:
        with open(config_module.USER_PREFERENCES_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip():
                logger.warning("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶å­˜åœ¨ä½†å†…å®¹ä¸ºç©ºã€‚")
                return []
            
            logger.info("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶å­˜åœ¨ä¸”éç©ºï¼Œå°è¯•è§£æJSON...")
            prefs = json.loads(content)
        
        selected = prefs.get("selected_categories", [])
        logger.info(f"è¯Šæ–­ï¼šæˆåŠŸè§£æJSONï¼Œæ‰¾åˆ° {len(selected)} æ¡åå¥½ã€‚")
        return selected

    except (IOError, json.JSONDecodeError) as e:
        logger.error(f"è¯Šæ–­ï¼šè¯»å–æˆ–è§£æåå¥½æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
        return []







def run_daily_workflow():
    """
    æ‰§è¡Œå®Œæ•´çš„æ¯æ—¥å·¥ä½œæµï¼ˆV3ç‰ˆï¼šé‡‡ç”¨é«˜çº§åˆ†ç±»æµç¨‹ï¼‰
    1. è·å–æ–°è®ºæ–‡ã€‚
    2. å¯¹æ–°è®ºæ–‡ä½¿ç”¨â€œç‹¬ç«‹åˆ†ç±»+RAGè¾…åŠ©å¯¹é½â€è¿›è¡Œé«˜è´¨é‡åˆ†ç±»ã€‚
    3. æ ¹æ®ç”¨æˆ·çš„ç²¾ç¡®åˆ†ç±»åå¥½è¿›è¡Œç­›é€‰ã€‚
    4. å¯¹ç­›é€‰åçš„è®ºæ–‡è¿›è¡Œå®Œæ•´çš„å¤„ç†å’Œå…¥åº“ã€‚
    5. ç”ŸæˆæŠ¥å‘Šã€‚
    """
    logger.info("ğŸš€ --- [V3.0 æ¯æ—¥å·¥ä½œæµå¯åŠ¨ - å«é«˜çº§RAGåˆ†ç±»] --- ğŸš€")
    
    current_config = config_module.get_current_config()
    logger.info(f"å½“å‰ä»»åŠ¡ä½¿ç”¨çš„æ¯æ—¥å¤„ç†ä¸Šé™ä¸º: {current_config['DAILY_PAPER_PROCESS_LIMIT']}")

    user_preferences = _get_user_preferences()
    if not user_preferences:
        logger.warning("ç”¨æˆ·æœªé€‰æ‹©ä»»ä½•åå¥½ç±»åˆ«ï¼Œå°†ä¸ä¼šå¤„ç†ä»»ä½•è®ºæ–‡ã€‚è¯·åœ¨UIä¸­è®¾ç½®åå¥½ã€‚")
        return {"message": "User preferences not set.", "papers_processed": 0}
    
    pref_set = set((item['domain'], item['task']) for item in user_preferences)
    logger.info(f"åŠ è½½äº† {len(pref_set)} æ¡ç”¨æˆ·åå¥½ã€‚")

    try:
        new_papers_data = arxiv_fetcher.fetch_daily_papers(
            domains=current_config["DEFAULT_ARXIV_DOMAINS"]
        )
        if not new_papers_data:
            logger.info("âœ… æœªå‘ç°æ–°è®ºæ–‡ï¼Œæ¯æ—¥ä»»åŠ¡ç»“æŸã€‚")
            return {"message": "No new papers today.", "papers_processed": 0}
        logger.info(f"å‘ç° {len(new_papers_data)} ç¯‡æ–°è®ºæ–‡ï¼Œå¼€å§‹è¿›è¡Œé«˜çº§åˆ†ç±»å’Œç­›é€‰...")
    except Exception as e:
        logger.critical(f"âŒ è·å–æ¯æ—¥è®ºæ–‡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return {"message": "Failed to fetch papers from arXiv.", "papers_processed": 0}

    papers_to_process = []
    for paper in new_papers_data:
        arxiv_id = paper['arxiv_id']
        if metadata_db.check_if_paper_exists(arxiv_id):
            continue

        # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨å…¨æ–°çš„é«˜çº§åˆ†ç±»å‡½æ•° â–¼â–¼â–¼
        final_classification = ingestion_agent.classify_paper_with_rag_context(paper['title'], paper['summary'])
        if not final_classification:
            logger.warning(f"æ— æ³•å¯¹è®ºæ–‡ {arxiv_id} è¿›è¡Œé«˜çº§åˆ†ç±»ï¼Œå·²è·³è¿‡ã€‚")
            continue
        # â–²â–²â–² ä¿®æ”¹ç»“æŸ â–²â–²â–²
        
        paper_category = (final_classification['domain'], final_classification['task'])
        if paper_category in pref_set:
            logger.info(f"ğŸ‘ è®ºæ–‡ {arxiv_id} æœ€ç»ˆåˆ†ç±»ä¸º {paper_category}ï¼ŒåŒ¹é…ç”¨æˆ·åå¥½ï¼ŒåŠ å…¥å¤„ç†é˜Ÿåˆ—ã€‚")
            paper['classification_result'] = final_classification
            papers_to_process.append(paper)
        else:
            logger.info(f"ğŸ‘ è®ºæ–‡ {arxiv_id} æœ€ç»ˆåˆ†ç±»ä¸º {paper_category}ï¼Œä¸åŒ¹é…ç”¨æˆ·åå¥½ï¼Œå·²è·³è¿‡ã€‚")
    
    if not papers_to_process:
        logger.info("âœ… ç­›é€‰å®Œæˆï¼Œæ²¡æœ‰è®ºæ–‡åŒ¹é…ç”¨æˆ·çš„åå¥½ç±»åˆ«ã€‚æ¯æ—¥ä»»åŠ¡ç»“æŸã€‚")
        return {"message": "No papers matched user preference.", "papers_processed": 0}
    
    logger.info(f"ç­›é€‰å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers_to_process)} ç¯‡åŒ¹é…ç”¨æˆ·åå¥½çš„è®ºæ–‡ã€‚")
    
    limit = current_config["DAILY_PAPER_PROCESS_LIMIT"]
    if len(papers_to_process) > limit:
        logger.warning(
            f"åŒ¹é…çš„è®ºæ–‡æ•° ({len(papers_to_process)}) è¶…è¿‡äº†è®¾å®šçš„ä¸Šé™ "
            f"({limit})ï¼Œå°†åªå¤„ç†å‰ {limit} ç¯‡ã€‚"
        )
        papers_to_process = papers_to_process[:limit]

    logger.info(f"æœ€ç»ˆå°†æœ‰ {len(papers_to_process)} ç¯‡è®ºæ–‡è¿›å…¥å¤„ç†æµç¨‹ã€‚")

    # è°ƒç”¨å®Œæ•´çš„å…¥åº“æµç¨‹
    successfully_processed_papers = process_papers_list(
        papers_to_process, 
        pdf_parsing_strategy=current_config["PDF_PARSING_STRATEGY"],
        ingestion_mode='full' # æ˜ç¡®æŒ‡å®šæ˜¯å®Œæ•´å…¥åº“
    )
    
    if vector_db.vector_db_manager and successfully_processed_papers:
        logger.info("æ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨ä¿å­˜å‘é‡ç´¢å¼•...")
        vector_db.vector_db_manager.save()
    
    _generate_daily_report(successfully_processed_papers)
    
    # åœ¨å·¥ä½œæµæœ€åï¼ŒåŒæ­¥ä¸€æ¬¡åˆ†ç±»ä½“ç³»ï¼Œç¡®ä¿UIæ˜¾ç¤ºæœ€æ–°
    ingestion_agent.export_categories_to_json()
    
    final_message = f"æ¯æ—¥å·¥ä½œæµå®Œæˆã€‚æˆåŠŸå¤„ç†å¹¶å…¥åº“ {len(successfully_processed_papers)} ç¯‡è®ºæ–‡ã€‚"
    logger.info(f"ğŸ --- [æ¯æ—¥å·¥ä½œæµç»“æŸ]: {final_message} --- ğŸ")
    
    return {
        "message": final_message,
        "papers_processed": len(successfully_processed_papers)
    }

def _generate_daily_report(processed_papers: List[Dict[str, Any]]):
    """ä¸ºå¤„ç†è¿‡çš„è®ºæ–‡ç”ŸæˆåŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„æ¯æ—¥æŠ¥å‘Šã€‚"""
    if not processed_papers:
        logger.info("æ²¡æœ‰æ–°å¤„ç†çš„è®ºæ–‡ï¼Œä¸ç”ŸæˆæŠ¥å‘Šã€‚")
        return

    logger.info(f"å‡†å¤‡ä¸º {len(processed_papers)} ç¯‡æ–°è®ºæ–‡ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š...")
    
    statistics = {}
    total_papers = len(processed_papers)
    for paper in processed_papers:
        classification = paper.get("classification_result", {})
        domain = classification.get("domain", "Unclassified")
        task = classification.get("task", "Unclassified")
        
        if domain not in statistics: statistics[domain] = {}
        if task not in statistics[domain]: statistics[domain][task] = 0
        statistics[domain][task] += 1

    report_jsons = []
    
    for paper_info in processed_papers:
        arxiv_id = paper_info["arxiv_id"]
        
        # ä»æ•°æ®åº“è·å–åŒ…å«AIç”Ÿæˆæ‘˜è¦çš„å®Œæ•´è®ºæ–‡è¯¦æƒ…
        paper_details = metadata_db.get_paper_details_by_id(arxiv_id)
        if not paper_details:
            logger.warning(f"æ— æ³•ä»æ•°æ®åº“è·å–è®ºæ–‡ {arxiv_id} çš„è¯¦ç»†ä¿¡æ¯ï¼Œè·³è¿‡æ­¤è®ºæ–‡çš„æŠ¥å‘Šç”Ÿæˆã€‚")
            continue
            
        # paper_details ç°åœ¨åŒ…å«äº† 'generated_summary' å­—æ®µ
        
        safe_arxiv_id = arxiv_id.replace('/', '_')
        structured_content_path = config_module.STRUCTURED_DATA_DIR / f"{safe_arxiv_id}.json"
        if not structured_content_path.exists(): 
            logger.warning(f"æ‰¾ä¸åˆ° {arxiv_id} çš„ç»“æ„åŒ–å†…å®¹æ–‡ä»¶ï¼Œè·³è¿‡æ­¤è®ºæ–‡çš„æŠ¥å‘Šç”Ÿæˆã€‚")
            continue
            
        with open(structured_content_path, 'r', encoding='utf-8') as f:
            structured_chunks = json.load(f)
            
        report_json_part = report_agent.generate_report_json_for_paper(
            paper_meta=paper_details, # ä¼ é€’å®Œæ•´çš„ã€æ›´æ–°åçš„è®ºæ–‡å…ƒæ•°æ®
            structured_chunks=structured_chunks
        )
        if report_json_part:
            report_jsons.append(report_json_part)
    # â–²â–²â–² ä¿®æ”¹ç»“æŸ â–²â–²â–²

    if not report_jsons:
        logger.error("æ‰€æœ‰è®ºæ–‡çš„æŠ¥å‘Šå†…å®¹éƒ½ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•åˆ›å»ºæ¯æ—¥æŠ¥å‘Šã€‚")
        return
    
    today_str = datetime.now().strftime("%Y-%m-%d")
    final_report_data = {
        "report_title": f"arXiv Daily Focus Report",
        "report_date": today_str,
        "statistics": {"total_papers": total_papers, "breakdown": statistics},
        "papers": report_jsons
    }
    
    report_filename_base = f"{config_module.DAILY_REPORT_PREFIX}_{today_str}"
    json_report_path = config_module.REPORTS_DIR / f"{report_filename_base}.json"
    pdf_report_path = config_module.REPORTS_DIR / f"{report_filename_base}.pdf"

    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(final_report_data, f, ensure_ascii=False, indent=4)
    logger.info(f"JSONæŠ¥å‘Šå·²ä¿å­˜: {json_report_path}")
    
    # åŠ¨æ€åˆ¤æ–­æŠ¥å‘Šè¯­è¨€
    report_language = 'zh' if "qwen3" in config_module.get_current_config()['OLLAMA_MODEL_NAME'].lower() else 'en'
    logger.info(f"Generating PDF report in '{report_language}' language.")
    pdf_generator.generate_daily_report_pdf(final_report_data, pdf_report_path, language=report_language)


def run_category_collection_workflow() -> Dict[str, Any]:
    """
    æ‰§è¡Œä¸€ä¸ªè½»é‡çº§çš„ç±»åˆ«æ”¶é›†å·¥ä½œæµï¼ˆV2ç‰ˆï¼šè½»é‡çº§å…¥åº“ï¼‰ã€‚
    è¯¥æµç¨‹ä»arXivè·å–è®ºæ–‡ï¼Œä½¿ç”¨LLMè¿›è¡Œé«˜è´¨é‡åˆ†ç±»ï¼Œç„¶ååªå°†è®ºæ–‡å…ƒæ•°æ®å’Œåˆ†ç±»
    ä¿¡æ¯å†™å…¥æ•°æ®åº“ï¼Œä¸å¤„ç†PDFï¼Œä»¥å®ç°å¿«é€Ÿçš„åˆ†ç±»ä½“ç³»æ‰©å……ã€‚
    """
    current_config = config_module.get_current_config()
    logger.info("--- [æ‰‹åŠ¨ç±»åˆ«æ”¶é›†å·¥ä½œæµå¯åŠ¨ (V2: è½»é‡çº§å…¥åº“)] ---")

    target_count = current_config['CATEGORY_COLLECTION_COUNT']
    years_window = current_config['CATEGORY_COLLECTION_YEARS_WINDOW']
    domains_query = " OR ".join([f"cat:{domain}" for domain in current_config['CATEGORY_COLLECTION_DOMAINS']])
    
    logger.info(f"ç›®æ ‡ï¼šä¸º {target_count} ç¯‡è®ºæ–‡è¿›è¡Œåˆ†ç±»å¹¶è½»é‡çº§å…¥åº“ã€‚")

    # ... [æ­¤éƒ¨åˆ†é‡‡æ ·é€»è¾‘ä¸æ—§ä»£ç ç›¸åŒï¼Œæ— éœ€æ›´æ”¹] ...
    now = datetime.now()
    all_possible_months = []
    for year in range(now.year - years_window + 1, now.year + 1):
        end_month = now.month if year == now.year else 12
        for month in range(1, end_month + 1):
            all_possible_months.append((year, month))
            
    if not all_possible_months:
        logger.warning("ç±»åˆ«æ”¶é›†ï¼šæœªèƒ½ç”Ÿæˆä»»ä½•å¯ä¾›é‡‡æ ·çš„å¹´æœˆèŒƒå›´ã€‚")
        return {"message": "æœªèƒ½ç”Ÿæˆä»»ä½•å¯ä¾›é‡‡æ ·çš„å¹´æœˆèŒƒå›´ã€‚", "categories_added": 0}

    papers_to_classify_and_ingest = []
    seen_ids = set()
    attempts = 0
    max_attempts = target_count * 15

    while len(papers_to_classify_and_ingest) < target_count and attempts < max_attempts:
        attempts += 1
        year, month = random.choice(all_possible_months)
        start_date = datetime(year, month, 1)
        end_date = (start_date + timedelta(days=32)).replace(day=1) - timedelta(days=1)
        
        date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')} TO {end_date.strftime('%Y%m%d')}]"
        full_query = f"({domains_query}) AND {date_query}"
        
        candidate_papers = arxiv_fetcher.search_arxiv(
            query=full_query, max_results=20, sort_by=arxiv.SortCriterion.Relevance
        )

        if not candidate_papers: continue

        random.shuffle(candidate_papers)
        for paper in candidate_papers:
            arxiv_id = paper['arxiv_id']
            if arxiv_id not in seen_ids and not metadata_db.check_if_paper_exists(arxiv_id):
                logger.info(f"ç±»åˆ«æ”¶é›†ï¼šæˆåŠŸé‡‡æ ·åˆ°æ–°è®ºæ–‡ {arxiv_id} (æ¥è‡ª {year}-{month})ã€‚ "
                            f"å½“å‰è¿›åº¦: {len(papers_to_classify_and_ingest) + 1}/{target_count}")
                papers_to_classify_and_ingest.append(paper)
                seen_ids.add(arxiv_id)
                if len(papers_to_classify_and_ingest) >= target_count:
                    break
        if len(papers_to_classify_and_ingest) >= target_count:
            break
    
    if not papers_to_classify_and_ingest:
        logger.warning("ç±»åˆ«æ”¶é›†ï¼šåœ¨æŒ‡å®šæ¬¡æ•°çš„å°è¯•ä¸­æœªèƒ½è·å–åˆ°ä»»ä½•æ–°çš„ã€ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")
        return {"message": "æœªèƒ½è·å–åˆ°ä»»ä½•æ–°çš„è®ºæ–‡ã€‚", "categories_added": 0}
        
    logger.info(f"é‡‡æ ·å®Œæˆï¼Œå°†å¯¹ {len(papers_to_classify_and_ingest)} ç¯‡æ–°è®ºæ–‡è¿›è¡Œé«˜çº§åˆ†ç±»å’Œè½»é‡çº§å…¥åº“...")

    # å¯¹é‡‡æ ·çš„è®ºæ–‡è¿›è¡Œåˆ†ç±»
    for paper in papers_to_classify_and_ingest:
        # ä½¿ç”¨æ–°çš„é«˜çº§åˆ†ç±»å‡½æ•°ï¼Œä¿è¯åˆ†ç±»è´¨é‡
        final_classification = ingestion_agent.classify_paper_with_rag_context(paper['title'], paper['summary'])
        if final_classification:
            paper['classification_result'] = final_classification
        else:
            # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œå¯ä»¥ç»™ä¸€ä¸ªé»˜è®¤å€¼æˆ–ç›´æ¥ä»åˆ—è¡¨ä¸­ç§»é™¤
            logger.warning(f"è®ºæ–‡ {paper['arxiv_id']} åˆ†ç±»å¤±è´¥ï¼Œå°†ä¸ä¼šè¢«å…¥åº“ã€‚")
    
    # è¿‡æ»¤æ‰åˆ†ç±»å¤±è´¥çš„è®ºæ–‡
    papers_to_ingest_lightly = [p for p in papers_to_classify_and_ingest if 'classification_result' in p]
    
    successfully_processed = process_papers_list(
        papers_to_ingest_lightly,
        ingestion_mode='metadata_only'
    )

    # åœ¨å·¥ä½œæµæœ€åï¼ŒåŒæ­¥ä¸€æ¬¡åˆ†ç±»ä½“ç³»ï¼Œç¡®ä¿UIæ˜¾ç¤ºæœ€æ–°
    ingestion_agent.export_categories_to_json()

    message = f"ç±»åˆ«æ”¶é›†å®Œæˆï¼æˆåŠŸä¸º {len(successfully_processed)} ç¯‡è®ºæ–‡æ·»åŠ äº†åˆ†ç±»å¹¶è½»é‡çº§å…¥åº“ã€‚"
    logger.info(f"--- [æ‰‹åŠ¨ç±»åˆ«æ”¶é›†å·¥ä½œæµç»“æŸ]: {message} ---")
    return {"message": message, "categories_added": len(successfully_processed)}
