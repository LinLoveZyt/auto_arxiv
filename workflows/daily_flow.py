# workflows/daily_flow.py
import random
import logging
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from core import config as config_module
from data_ingestion import arxiv_fetcher
from hrag import metadata_db, vector_db
from workflows.ingestion_flow import process_papers_list
from agents import report_agent, ingestion_agent
from utils import pdf_generator
import arxiv

logger = logging.getLogger(__name__)


def _get_user_preferences() -> List[Dict[str, str]]:
    """ä»JSONæ–‡ä»¶åŠ è½½ç”¨æˆ·é€‰æ‹©çš„åå¥½ç±»åˆ«ã€‚"""
    logger.info(f"æ­£åœ¨æ£€æŸ¥ç”¨æˆ·åå¥½æ–‡ä»¶: {config_module.USER_PREFERENCES_PATH}")
    
    if not config_module.USER_PREFERENCES_PATH.exists():
        logger.warning("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶ user_preferences.json ä¸å­˜åœ¨ã€‚")
        return []
        
    try:
        with open(config_module.USER_PREFERENCES_PATH, 'r', encoding='utf-8') as f:
            content = f.read()
            if not content.strip():
                logger.warning("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶å­˜åœ¨ä½†å†…å®¹ä¸ºç©ºã€‚")
                return []
            
            logger.info("è¯Šæ–­ï¼šåå¥½æ–‡ä»¶å­˜åœ¨ä¸”éç©ºï¼Œå°è¯•è§£æJSON...")
            prefs = json.loads(content)
        
        selected = prefs.get("selected_categories", [])
        logger.info(f"è¯Šæ–­ï¼šæˆåŠŸè§£æJSONï¼Œæ‰¾åˆ° {len(selected)} æ¡åå¥½ã€‚")
        return selected

    except (IOError, json.JSONDecodeError) as e:
        logger.error(f"è¯Šæ–­ï¼šè¯»å–æˆ–è§£æåå¥½æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
        return []


def _run_cold_start_population():
    """
    å†·å¯åŠ¨å‡½æ•°ã€‚é€šè¿‡å¾ªç¯Næ¬¡ï¼Œæ¯æ¬¡éƒ½ä»è¿‡å»å‡ å¹´å†…éšæœºé€‰æ‹©ä¸€ä¸ªå¹´æœˆï¼Œ
    å¹¶ä»è¯¥æœˆä¸­éšæœºè·å–ä¸€ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œä»¥å»ºç«‹ä¸€ä¸ªå¤šæ ·åŒ–ä¸”ç°ä»£çš„åˆ†ç±»ä½“ç³»ã€‚
    """
    current_config = config_module.get_current_config()
    logger.info("--- [å†·å¯åŠ¨æœºåˆ¶è§¦å‘] ---")
    logger.info("æ•°æ®åº“ä¸­è®ºæ–‡è¿‡å°‘ï¼Œå¼€å§‹é€ç¯‡éšæœºé‡‡æ ·ä»¥å»ºç«‹åˆ†ç±»ä½“ç³»ã€‚")

    papers_to_process = []
    seen_ids = set()
    target_count = current_config['COLD_START_PAPER_COUNT']
    years_window = current_config['COLD_START_YEARS_WINDOW']
    domains_query = " OR ".join([f"cat:{domain}" for domain in current_config['COLD_START_DOMAINS']])
    
    now = datetime.now()
    all_possible_months = []
    for year in range(now.year - years_window + 1, now.year + 1):
        end_month = now.month if year == now.year else 12
        for month in range(1, end_month + 1):
            all_possible_months.append((year, month))
            
    if not all_possible_months:
        logger.warning("å†·å¯åŠ¨ï¼šæœªèƒ½ç”Ÿæˆä»»ä½•å¯ä¾›é‡‡æ ·çš„å¹´æœˆèŒƒå›´ã€‚")
        return

    attempts = 0
    max_attempts = target_count * 10 

    while len(papers_to_process) < target_count and attempts < max_attempts:
        attempts += 1
        
        year, month = random.choice(all_possible_months)
        
        start_date = datetime(year, month, 1)
        if month == 12:
            end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
        else:
            end_date = datetime(year, month + 1, 1) - timedelta(days=1)
        
        date_query = f"submittedDate:[{start_date.strftime('%Y%m%d')} TO {end_date.strftime('%Y%m%d')}]"
        full_query = f"({domains_query}) AND {date_query}"
        
        candidate_papers = arxiv_fetcher.search_arxiv(
            query=full_query,
            max_results=20, 
            sort_by=arxiv.SortCriterion.Relevance
        )

        if not candidate_papers:
            logger.info(f"åœ¨ {year}-{month} æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡ï¼Œå°è¯•ä¸‹ä¸€ä¸ªéšæœºå¹´æœˆã€‚")
            continue

        random.shuffle(candidate_papers)
        for paper in candidate_papers:
            arxiv_id = paper['arxiv_id']
            if arxiv_id not in seen_ids and not metadata_db.check_if_paper_exists(arxiv_id):
                logger.info(f"å†·å¯åŠ¨ï¼šæˆåŠŸé‡‡æ ·åˆ°æ–°è®ºæ–‡ {arxiv_id} (æ¥è‡ª {year}-{month})ã€‚ "
                            f"å½“å‰è¿›åº¦: {len(papers_to_process) + 1}/{target_count}")
                papers_to_process.append(paper)
                seen_ids.add(arxiv_id)
                break
    
    if not papers_to_process:
        logger.warning("å†·å¯åŠ¨ï¼šåœ¨æŒ‡å®šæ¬¡æ•°çš„å°è¯•ä¸­æœªèƒ½è·å–åˆ°ä»»ä½•æ–°çš„ã€ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")
        return
        
    logger.info(f"å†·å¯åŠ¨é‡‡æ ·å®Œæˆï¼Œå°†å¤„ç† {len(papers_to_process)} ç¯‡æ–°è®ºæ–‡ã€‚")

    process_papers_list(papers_to_process, pdf_parsing_strategy=current_config['PDF_PARSING_STRATEGY'])
    logger.info("--- [å†·å¯åŠ¨æœºåˆ¶å®Œæˆ] ---")



def run_daily_workflow():
    """
    æ‰§è¡Œå®Œæ•´çš„æ¯æ—¥å·¥ä½œæµï¼š
    1. (å¯é€‰) æ‰§è¡Œå†·å¯åŠ¨ä»¥å¡«å……åˆ†ç±»ã€‚
    2. è·å–æ–°è®ºæ–‡ã€‚
    3. å¯¹æ–°è®ºæ–‡è¿›è¡Œç‹¬ç«‹åˆ†ç±»ï¼Œç„¶åè¿›è¡Œåˆ†ç±»å¯¹é½ã€‚
    4. æ ¹æ®ç”¨æˆ·çš„ç²¾ç¡®åˆ†ç±»åå¥½è¿›è¡Œç­›é€‰ã€‚
    5. å¤„ç†å¹¶å…¥åº“ç­›é€‰åçš„è®ºæ–‡ã€‚
    6. ç”ŸæˆæŠ¥å‘Šã€‚
    """
    logger.info("ğŸš€ --- [V2.1 æ¯æ—¥å·¥ä½œæµå¯åŠ¨ - å«åˆ†ç±»å¯¹é½] --- ğŸš€")
    
    current_config = config_module.get_current_config()
    logger.info(f"å½“å‰ä»»åŠ¡ä½¿ç”¨çš„æ¯æ—¥å¤„ç†ä¸Šé™ä¸º: {current_config['DAILY_PAPER_PROCESS_LIMIT']}")

    if metadata_db.get_total_paper_count() < current_config["COLD_START_PAPER_COUNT"]:
        _run_cold_start_population()

    user_preferences = _get_user_preferences()
    if not user_preferences:
        logger.warning("ç”¨æˆ·æœªé€‰æ‹©ä»»ä½•åå¥½ç±»åˆ«ï¼Œå°†ä¸ä¼šå¤„ç†ä»»ä½•è®ºæ–‡ã€‚è¯·åœ¨UIä¸­è®¾ç½®åå¥½ã€‚")
        return {"message": "User preferences not set.", "papers_processed": 0}
    
    pref_set = set((item['domain'], item['task']) for item in user_preferences)
    known_categories = ingestion_agent.get_known_categories()
    logger.info(f"åŠ è½½äº† {len(pref_set)} æ¡ç”¨æˆ·åå¥½å’Œ {len(known_categories)} ä¸ªå·²çŸ¥é¢†åŸŸã€‚")

    try:
        new_papers_data = arxiv_fetcher.fetch_daily_papers(
            domains=current_config["DEFAULT_ARXIV_DOMAINS"]
        )
        if not new_papers_data:
            logger.info("âœ… æœªå‘ç°æ–°è®ºæ–‡ï¼Œæ¯æ—¥ä»»åŠ¡ç»“æŸã€‚")
            return {"message": "No new papers today.", "papers_processed": 0}
        logger.info(f"å‘ç° {len(new_papers_data)} ç¯‡æ–°è®ºæ–‡ï¼Œå¼€å§‹è¿›è¡Œåˆ†ç±»ã€å¯¹é½å’Œç­›é€‰...")
    except Exception as e:
        logger.critical(f"âŒ è·å–æ¯æ—¥è®ºæ–‡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return {"message": "Failed to fetch papers from arXiv.", "papers_processed": 0}

    papers_to_process = []
    for paper in new_papers_data:
        arxiv_id = paper['arxiv_id']
        if metadata_db.check_if_paper_exists(arxiv_id):
            continue

        raw_classification = ingestion_agent.classify_paper(paper['title'], paper['summary'])
        if not raw_classification:
            logger.warning(f"æ— æ³•å¯¹è®ºæ–‡ {arxiv_id} è¿›è¡Œåˆæ­¥åˆ†ç±»ï¼Œå·²è·³è¿‡ã€‚")
            continue
        
        aligned_result = ingestion_agent.align_classification(raw_classification, known_categories)
        if not aligned_result:
            logger.warning(f"å¯¹é½è®ºæ–‡ {arxiv_id} çš„åˆ†ç±»å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚")
            continue

        # vvv [ä¿®æ”¹] ä½¿ç”¨æ­£ç¡®çš„é”®åå¹¶æ›´æ–°åˆ†ç±»ä½“ç³» vvv
        final_domain = aligned_result["final_domain"]
        final_task = aligned_result["final_task"]
        
        # å°†å¯¹é½åçš„ã€æ ‡å‡†åŒ–çš„åˆ†ç±»æ›´æ–°åˆ°å…¨å±€åˆ†ç±»æ–‡ä»¶ä¸­
        ingestion_agent._update_known_categories(final_domain, final_task)

        final_classification = {
            "domain": final_domain,
            "task": final_task
        }
        # ^^^ [ä¿®æ”¹] ^^^
        
        paper_category = (final_classification['domain'], final_classification['task'])
        if paper_category in pref_set:
            logger.info(f"ğŸ‘ è®ºæ–‡ {arxiv_id} æœ€ç»ˆåˆ†ç±»ä¸º {paper_category}ï¼ŒåŒ¹é…ç”¨æˆ·åå¥½ï¼ŒåŠ å…¥å¤„ç†é˜Ÿåˆ—ã€‚")
            paper['classification_result'] = final_classification
            papers_to_process.append(paper)
        else:
            logger.info(f"ğŸ‘ è®ºæ–‡ {arxiv_id} æœ€ç»ˆåˆ†ç±»ä¸º {paper_category}ï¼Œä¸åŒ¹é…ç”¨æˆ·åå¥½ï¼Œå·²è·³è¿‡ã€‚")
    
    if not papers_to_process:
        logger.info("âœ… ç­›é€‰å®Œæˆï¼Œæ²¡æœ‰è®ºæ–‡åŒ¹é…ç”¨æˆ·çš„åå¥½ç±»åˆ«ã€‚æ¯æ—¥ä»»åŠ¡ç»“æŸã€‚")
        return {"message": "No papers matched user preference.", "papers_processed": 0}
    
    logger.info(f"ç­›é€‰å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers_to_process)} ç¯‡åŒ¹é…ç”¨æˆ·åå¥½çš„è®ºæ–‡ã€‚")
    
    limit = current_config["DAILY_PAPER_PROCESS_LIMIT"]
    if len(papers_to_process) > limit:
        logger.warning(
            f"åŒ¹é…çš„è®ºæ–‡æ•° ({len(papers_to_process)}) è¶…è¿‡äº†è®¾å®šçš„ä¸Šé™ "
            f"({limit})ï¼Œå°†åªå¤„ç†å‰ {limit} ç¯‡ã€‚"
        )
        papers_to_process = papers_to_process[:limit]

    logger.info(f"æœ€ç»ˆå°†æœ‰ {len(papers_to_process)} ç¯‡è®ºæ–‡è¿›å…¥å¤„ç†æµç¨‹ã€‚")

    successfully_processed_papers = process_papers_list(
        papers_to_process, 
        pdf_parsing_strategy=current_config["PDF_PARSING_STRATEGY"]
    )
    
    if vector_db.vector_db_manager and successfully_processed_papers:
        logger.info("æ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨ä¿å­˜å‘é‡ç´¢å¼•...")
        vector_db.vector_db_manager.save()
    
    _generate_daily_report(successfully_processed_papers)
    
    final_message = f"æ¯æ—¥å·¥ä½œæµå®Œæˆã€‚æˆåŠŸå¤„ç†å¹¶å…¥åº“ {len(successfully_processed_papers)} ç¯‡è®ºæ–‡ã€‚"
    logger.info(f"ğŸ --- [æ¯æ—¥å·¥ä½œæµç»“æŸ]: {final_message} --- ğŸ")
    
    return {
        "message": final_message,
        "papers_processed": len(successfully_processed_papers)
    }


def _prepare_report_context(structured_chunks: List[Dict[str, Any]], hrag_manager: 'HRAGManager', arxiv_id: str) -> Dict[str, Any]:
    """
    Prepares a concise and informative context for the report generation agent.
    This includes the paper's summary and a list of images/tables with their surrounding context.
    """
    # 1. è·å–å·²ç”Ÿæˆçš„é«˜è´¨é‡æ‘˜è¦
    paper_summary = hrag_manager.metadata_db.get_paper_summary_by_id(arxiv_id)
    if not paper_summary:
        logger.warning(f"Could not retrieve summary for {arxiv_id}, report quality may be affected.")
        paper_summary = "Summary not available."

    # 2. æå–å›¾ç‰‡å’Œè¡¨æ ¼ï¼Œå¹¶æ•è·å…¶ä¸Šä¸‹æ–‡
    media_with_context = []
    text_chunks = [chunk['text'] for chunk in structured_chunks if chunk['type'] == 'text']
    
    for i, chunk in enumerate(structured_chunks):
        chunk_type = chunk.get('type')
        if chunk_type in ['image', 'table']:
            # æå–æ ‡é¢˜
            caption = chunk.get('metadata', {}).get('caption', 'No caption available.')
            # æå–æ–‡ä»¶è·¯å¾„ (é’ˆå¯¹å›¾ç‰‡)
            image_path = chunk.get('metadata', {}).get('image_path', '')

            # å¯»æ‰¾ä¸Šä¸‹æ–‡ï¼šå‘å‰å’Œå‘åæŸ¥æ‰¾æœ€è¿‘çš„æ–‡æœ¬å—
            context_before = ""
            context_after = ""
            
            # å‘å‰æŸ¥æ‰¾
            for j in range(i - 1, -1, -1):
                if structured_chunks[j].get('type') == 'text':
                    context_before = structured_chunks[j].get('text', '')
                    break
            
            # å‘åæŸ¥æ‰¾
            for j in range(i + 1, len(structured_chunks)):
                if structured_chunks[j].get('type') == 'text':
                    context_after = structured_chunks[j].get('text', '')
                    break
            
            formatted_item = (
                f"- Type: {chunk_type.capitalize()}\n"
                f"  Caption: {caption}\n"
            )
            if image_path:
                formatted_item += f"  Image Path: {image_path}\n"
            
            formatted_item += (
                f"  Context Before: \"...{context_before[-200:]}\"\n"  # æœ€å¤šæˆªå–å‰æ–‡çš„å200ä¸ªå­—ç¬¦
                f"  Context After: \"{context_after[:200]}...\""      # æœ€å¤šæˆªå–åæ–‡çš„å‰200ä¸ªå­—ç¬¦
            )
            media_with_context.append(formatted_item)

    media_summary_str = "\n\n".join(media_with_context) if media_with_context else "No images or tables found in the document."

    return {
        "paper_summary": paper_summary,
        "media_summary_str": media_summary_str
    }

def _generate_daily_report(hrag_manager: 'HRAGManager', successful_papers: List[Dict[str, Any]]):
    """Generates and saves a daily report for all newly processed papers."""
    if not successful_papers:
        logger.info("No new papers to generate a report for.")
        return

    logger.info(f"Preparing to generate daily report for {len(successful_papers)} new papers...")
    
    report_agent = ReportAgent()
    report_data_list = []

    for paper_data in successful_papers:
        arxiv_id = paper_data['arxiv_id']
        structured_data_path = paper_data['structured_data_path']

        try:
            with open(structured_data_path, 'r') as f:
                structured_chunks = json.load(f)

            # --- è¿™æ˜¯æ ¸å¿ƒä¿®æ”¹ ---
            # å‡†å¤‡ä¸€ä¸ªç²¾ç®€ã€é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯ä¼ é€’æ•´ä¸ªï¼ˆå¯èƒ½è¢«æˆªæ–­çš„ï¼‰æ–‡ä»¶å†…å®¹
            report_context = _prepare_report_context(structured_chunks, hrag_manager, arxiv_id)
            
            # å°†å‡†å¤‡å¥½çš„ä¸Šä¸‹æ–‡ä¼ é€’ç»™ report_agent
            report_json = report_agent.generate_report_json_for_paper(
                paper_meta=paper_data, 
                report_context=report_context
            )
            # --------------------

            if report_json:
                report_data_list.append(report_json)

        except FileNotFoundError:
            logger.error(f"Structured data file not found for {arxiv_id} at {structured_data_path}. Skipping for report generation.")
        except Exception as e:
            logger.error(f"Failed to generate report section for {arxiv_id}: {e}", exc_info=True)

    if not report_data_list:
        logger.warning("No report data could be generated for any of the papers.")
        return

    # ä¿å­˜ JSON å’Œ PDF æŠ¥å‘Šçš„é€»è¾‘ä¿æŒä¸å˜
    # ... (è¿™éƒ¨åˆ†é€»è¾‘å’ŒåŸæ¥ä¸€æ ·) ...
    storage_path = current_config.get('storage_path', 'storage')
    report_dir = os.path.join(storage_path, 'reports')
    os.makedirs(report_dir, exist_ok=True)
    
    today_str = datetime.now().strftime('%Y-%m-%d')
    json_report_path = os.path.join(report_dir, f"Daily_arXiv_Report_{today_str}.json")

    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(report_data_list, f, indent=2, ensure_ascii=False)
    logger.info(f"JSON report saved to: {json_report_path}")

    try:
        pdf_generator = PDFGenerator()
        pdf_generator.generate_from_json(json_report_path)
    except Exception as e:
        logger.error(f"An unexpected error occurred during PDF generation: {e}", exc_info=True)