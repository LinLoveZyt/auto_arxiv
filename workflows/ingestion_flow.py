# workflows/ingestion_flow.py

import logging
import json
from typing import List, Dict, Any

from hrag import metadata_db
from data_ingestion import pdf_processor
from hrag import hrag_manager as hrag_manager_module
import time
import gc
import torch

logger = logging.getLogger(__name__)

def process_papers_list(
    papers_to_process: List[Dict[str, Any]],
    pdf_parsing_strategy: str = "monkey"  # é»˜è®¤ä½¿ç”¨ 'monkey'
) -> List[Dict[str, Any]]:
    """
    å¤„ç†ä¸€ä¸ªç»™å®šçš„è®ºæ–‡ä¿¡æ¯å­—å…¸åˆ—è¡¨ï¼Œå°†å®ƒä»¬ä¸‹è½½ã€è§£æå¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­ã€‚
    è¿™æ˜¯ä¸€ä¸ªå¯å¤ç”¨çš„æ ¸å¿ƒå·¥ä½œæµã€‚

    Args:
        papers_to_process: ä¸€ä¸ªåŒ…å«å¤šç¯‡è®ºæ–‡ä¿¡æ¯çš„åˆ—è¡¨ã€‚
        pdf_parsing_strategy: ä½¿ç”¨çš„PDFè§£æç­–ç•¥ ('monkey' æˆ– 'fast')ã€‚

    Returns:
        ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«è¢«æˆåŠŸå¤„ç†å¹¶å…¥åº“çš„è®ºæ–‡ä¿¡æ¯ã€‚
    """
    if not papers_to_process:
        return []

    logger.info(f"--- [é€šç”¨å…¥åº“æµç¨‹å¯åŠ¨]ï¼šå‡†å¤‡å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ '{pdf_parsing_strategy}' è§£æç­–ç•¥ ---")
    
    successfully_processed_papers = []
    
    for i, paper_data in enumerate(papers_to_process, 1):
        arxiv_id = paper_data["arxiv_id"]
        title = paper_data["title"]
        
        logger.info(f"--- [è¿›åº¦ {i}/{len(papers_to_process)}] å¼€å§‹å¤„ç†è®ºæ–‡: {arxiv_id} - {title[:60]}... ---")

        try:
            if metadata_db.check_if_paper_exists(arxiv_id):
                logger.info(f"è®ºæ–‡ {arxiv_id} å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œè·³è¿‡ã€‚")
                successfully_processed_papers.append(paper_data)
                continue

            # â–¼â–¼â–¼ [ä¿®æ”¹] å°†è§£æç­–ç•¥ä½œä¸ºå‚æ•°ä¼ é€’ â–¼â–¼â–¼
            path_info = pdf_processor.process_paper(paper_data, strategy=pdf_parsing_strategy)
            if not path_info:
                logger.error(f"å¤„ç†è®ºæ–‡ {arxiv_id} çš„PDFå¤±è´¥ï¼Œè·³è¿‡æ­¤è®ºæ–‡ã€‚")
                continue
            
            # ... å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ ...
            full_paper_data = {**paper_data, **path_info}
            paper_db_id = metadata_db.add_paper(full_paper_data)
            if paper_db_id is None:
                logger.error(f"æ— æ³•å°†è®ºæ–‡ {arxiv_id} çš„å…ƒæ•°æ®åˆå§‹å†™å…¥æ•°æ®åº“ï¼Œè·³è¿‡ã€‚")
                continue

            with open(path_info["json_path"], 'r', encoding='utf-8') as f:
                structured_chunks = json.load(f)

            success = hrag_manager_module.hrag_manager.process_and_add_paper(
                paper_data, 
                structured_chunks,
                classification=paper_data.get('classification_result')
            )
            if success:
                successfully_processed_papers.append(paper_data)
                logger.info(f"ğŸ‰ è®ºæ–‡ {arxiv_id} å·²æˆåŠŸå®Œæˆæ‰€æœ‰å¤„ç†å¹¶å…¥åº“ï¼")
            else:
                logger.error(f"H-RAGç®¡ç†å™¨æœªèƒ½æˆåŠŸå¤„ç†è®ºæ–‡ {arxiv_id}ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ—¥å¿—ã€‚")

        except Exception as e:
            logger.critical(f"âŒ å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿæ„å¤–çš„ä¸¥é‡é”™è¯¯ï¼Œè·³è¿‡æ­¤è®ºæ–‡: {e}", exc_info=True)
            continue
        
        finally:
            logger.info(f"--- [å¾ªç¯é—´éš™æ¸…ç†] æ­£åœ¨ä¸ºä¸‹ä¸€ç¯‡è®ºæ–‡å‡†å¤‡ç¯å¢ƒ... ---")
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            time.sleep(1)

    logger.info(f"--- [é€šç”¨å…¥åº“æµç¨‹ç»“æŸ]ï¼šæˆåŠŸå¤„ç† {len(successfully_processed_papers)} / {len(papers_to_process)} ç¯‡è®ºæ–‡ ---")
    return successfully_processed_papers