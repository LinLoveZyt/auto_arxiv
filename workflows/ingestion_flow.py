# workflows/ingestion_flow.py

import logging
import json
from typing import List, Dict, Any

from hrag import metadata_db
from data_ingestion import pdf_processor
from hrag import hrag_manager as hrag_manager_module
import time
import gc
import torch

logger = logging.getLogger(__name__)

def process_papers_list(
    papers_to_process: List[Dict[str, Any]],
    pdf_parsing_strategy: str = "monkey",
    ingestion_mode: str = 'full'  # æ–°å¢å‚æ•°: 'full' æˆ– 'metadata_only'
) -> List[Dict[str, Any]]:
    """
    å¤„ç†ä¸€ä¸ªç»™å®šçš„è®ºæ–‡ä¿¡æ¯å­—å…¸åˆ—è¡¨ã€‚
    è¿™æ˜¯ä¸€ä¸ªå¯å¤ç”¨çš„æ ¸å¿ƒå·¥ä½œæµï¼Œç°åœ¨æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - 'full': ä¸‹è½½ã€è§£æPDFï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬å‘é‡ï¼‰æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    - 'metadata_only': åªå°†è®ºæ–‡çš„å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€åˆ†ç±»ç­‰ï¼‰æ·»åŠ åˆ°æ•°æ®åº“ï¼Œä¸å¤„ç†PDFå’Œå‘é‡ã€‚
    """
    if not papers_to_process:
        return []

    logger.info(f"--- [é€šç”¨å…¥åº“æµç¨‹å¯åŠ¨]ï¼šå‡†å¤‡å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ '{ingestion_mode}' æ¨¡å¼ ---")
    
    successfully_processed_papers = []
    
    for i, paper_data in enumerate(papers_to_process, 1):
        arxiv_id = paper_data["arxiv_id"]
        title = paper_data["title"]
        classification_result = paper_data.get('classification_result') # è·å–åˆ†ç±»ç»“æœ

        logger.info(f"--- [è¿›åº¦ {i}/{len(papers_to_process)}] å¼€å§‹å¤„ç†è®ºæ–‡: {arxiv_id} - {title[:60]}... ---")

        try:
            if metadata_db.check_if_paper_exists(arxiv_id):
                logger.info(f"è®ºæ–‡ {arxiv_id} å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œè·³è¿‡ã€‚")
                # å³ä½¿è·³è¿‡ï¼Œä¹Ÿè®¤ä¸ºå®ƒæ˜¯â€œæˆåŠŸâ€çš„ï¼Œå› ä¸ºå®ƒå·²åœ¨åº“ä¸­
                successfully_processed_papers.append(paper_data)
                continue

            # --- æ¨¡å¼åˆ†æ”¯ ---
            if ingestion_mode == 'full':
                # å®Œæ•´çš„PDFå¤„ç†å’Œå…¥åº“æµç¨‹
                path_info = pdf_processor.process_paper(paper_data, strategy=pdf_parsing_strategy)
                if not path_info:
                    logger.error(f"å¤„ç†è®ºæ–‡ {arxiv_id} çš„PDFå¤±è´¥ï¼Œè·³è¿‡æ­¤è®ºæ–‡ã€‚")
                    continue
                
                full_paper_data = {**paper_data, **path_info}
                paper_db_id = metadata_db.add_paper(full_paper_data)
                if paper_db_id is None:
                    logger.error(f"æ— æ³•å°†è®ºæ–‡ {arxiv_id} çš„å…ƒæ•°æ®åˆå§‹å†™å…¥æ•°æ®åº“ï¼Œè·³è¿‡ã€‚")
                    continue

                with open(path_info["json_path"], 'r', encoding='utf-8') as f:
                    structured_chunks = json.load(f)

                success = hrag_manager_module.hrag_manager.process_and_add_paper(
                    paper_data, 
                    structured_chunks,
                    classification=classification_result
                )
                if success:
                    successfully_processed_papers.append(paper_data)
                    logger.info(f"ğŸ‰ è®ºæ–‡ {arxiv_id} å·²æˆåŠŸå®Œæˆæ‰€æœ‰å¤„ç†å¹¶å…¥åº“ï¼")
                else:
                    logger.error(f"H-RAGç®¡ç†å™¨æœªèƒ½æˆåŠŸå¤„ç†è®ºæ–‡ {arxiv_id}ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ—¥å¿—ã€‚")

            elif ingestion_mode == 'metadata_only':
                # è½»é‡çº§çš„å…ƒæ•°æ®å…¥åº“æµç¨‹
                logger.info(f"æ‰§è¡Œè½»é‡çº§å…¥åº“ï¼Œåªä¿å­˜è®ºæ–‡ {arxiv_id} çš„å…ƒæ•°æ®å’Œåˆ†ç±»ä¿¡æ¯ã€‚")
                
                # 1. æ·»åŠ è®ºæ–‡å…ƒæ•°æ®ï¼Œè·¯å¾„å­—æ®µä¸ºç©º
                # æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªç©ºçš„ path_infoï¼Œç¡®ä¿ add_paper å‡½æ•°æœ‰ä¸œè¥¿è§£åŒ…
                path_info = {"pdf_path": None, "json_path": None}
                full_paper_data = {**paper_data, **path_info}
                paper_db_id = metadata_db.add_paper(full_paper_data)
                if paper_db_id is None:
                    logger.error(f"æ— æ³•å°†è®ºæ–‡ {arxiv_id} çš„å…ƒæ•°æ®å†™å…¥æ•°æ®åº“ï¼Œè·³è¿‡ã€‚")
                    continue
                
                # 2. å¦‚æœæœ‰åˆ†ç±»ç»“æœï¼Œåˆ™æ›´æ–°è®ºæ–‡çš„åˆ†ç±»
                if classification_result:
                    conn = metadata_db.get_db_connection()
                    try:
                        with conn:
                            domain_id = metadata_db.add_or_get_domain(classification_result["domain"], conn=conn)
                            task_id = metadata_db.add_or_get_task(classification_result["task"], domain_id, conn=conn)
                            # åªæ›´æ–°åˆ†ç±»ï¼Œæ‘˜è¦å­—æ®µä¼ None
                            metadata_db.update_paper_summary_and_classification(
                                arxiv_id=arxiv_id, domain_id=domain_id, task_id=task_id, summary=None, conn=conn
                            )
                        logger.info(f"è®ºæ–‡ {arxiv_id} çš„åˆ†ç±»ä¿¡æ¯å·²æ›´æ–°ã€‚")
                    finally:
                        if conn: conn.close()
                
                successfully_processed_papers.append(paper_data)
                logger.info(f"ğŸ‰ è®ºæ–‡ {arxiv_id} çš„å…ƒæ•°æ®å·²æˆåŠŸå…¥åº“ï¼")
            
            else:
                 logger.error(f"æœªçŸ¥çš„å…¥åº“æ¨¡å¼: '{ingestion_mode}'ï¼Œè·³è¿‡è®ºæ–‡ {arxiv_id}ã€‚")
                 continue

        except Exception as e:
            logger.critical(f"âŒ å¤„ç†è®ºæ–‡ {arxiv_id} æ—¶å‘ç”Ÿæ„å¤–çš„ä¸¥é‡é”™è¯¯ï¼Œè·³è¿‡æ­¤è®ºæ–‡: {e}", exc_info=True)
            continue
        
        finally:
            logger.info(f"--- [å¾ªç¯é—´éš™æ¸…ç†] æ­£åœ¨ä¸ºä¸‹ä¸€ç¯‡è®ºæ–‡å‡†å¤‡ç¯å¢ƒ... ---")
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            time.sleep(1)

    logger.info(f"--- [é€šç”¨å…¥åº“æµç¨‹ç»“æŸ]ï¼šæˆåŠŸå¤„ç† {len(successfully_processed_papers)} / {len(papers_to_process)} ç¯‡è®ºæ–‡ ---")
    return successfully_processed_papers
